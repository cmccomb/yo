#!/usr/bin/env zsh

# `yo` - A command-line AI assistant
function yo() {

	# Symbol used to signal the end of one-off sessions
	local STOP_SYMBOL="✌️"

	# Function to display usage instructions
	function show_help() {
		echo "
yo - A command-line AI assistant.

Usage:
  yo [options] [question]

Description:
  If a question is provided, Yo will answer the question. Otherwise, Yo will enter an interactive session.

Options:
  -h, --help          Show this help message and exit.
  -r, --read [file]   Use RAG functionality to integrate information from the specified file (supports text files and PDFs).
  -v, --verbose       Enable verbose mode for detailed output.
"
	}

	# Function to generate a prompt for one-off question-answering
	function generate_one_off_prompt() {
		local query=$1
		local content=$2
		local filename=$3
		local prompt="
You are playing the role of Yo, a helpful assistant that interacts with users in the MacOS terminal through one-off answers.

"
		# Include file content and name if provided
		if [[ -n "$content" ]]; then
			prompt+="Relevant Information from file \"$filename\":
================= BEGINNING OF FILE CONTENTS =================
$content
==================== END OF FILE CONTENTS ====================

"
		fi

		prompt+="Your task is to answer the user's question efficiently and accurately.
Your answer will be shown as raw text in a terminal window.
Your answer should be as short as possible.
Your answer should end with the stop symbol '$STOP_SYMBOL' to return control to the user.
User Query: $query $STOP_SYMBOL
Your Super-Short Answer:"
		echo "$prompt"
	}

	# Function to generate a prompt for interactive chat sessions
	function generate_interactive_prompt() {
		local content=$1
		local filename=$2
		local prompt="
You are playing the role of Yo.
Yo is a helpful assistant that interacts with a user through an interactive chat interface in the terminal.
The current time and date is: $(date).

"

		# Include file content and name if provided
		if [[ -n "$content" ]]; then
			prompt+="Relevant Information from file \"$filename\":
================= BEGINNING OF FILE CONTENTS =================
$content
==================== END OF FILE CONTENTS ====================

"
		fi

		prompt+="Your task is to answer the user's questions efficiently and accurately.
Your responses will be shown as raw text in a terminal window.
Your responses should be concise but complete.
"
		echo "$prompt"
	}

	# Function to extract content from a file (supports text and PDF files)
	function extract_content() {
		local file=$1
		local truncate_length=$2

		if [[ -f "$file" ]]; then
			local content
			case $file in
			*.pdf)
				# Extract text from PDF files using pdftotext
				if command -v pdftotext >/dev/null 2>&1; then
					content=$(pdftotext "$file" - 2>/dev/null)
				else
					echo "Error: pdftotext is not installed. Install it using your package manager (e.g., brew install poppler)." >&2
					return 1
				fi
				;;
			*.txt | *)
				# Extract text from plain text files
				content=$(cat "$file")
				;;
			esac

			# Truncate content if a length limit is provided
			if [[ -n "$truncate_length" && "$truncate_length" -gt 0 ]]; then
				content=${content:0:$truncate_length}
			fi

			echo "$content"
		else
			echo "Error: File not found or invalid file specified." >&2
			return 1
		fi
	}

	# Function to start a llama-cli session
	function start_llama_session() {
		local size=$1
		local HF_USERNAME="bartowski" # Hugging Face username
		local SERIES="Qwen2.5"        # Model series
		local TUNING="Instruct"       # Model tuning
		local TYPE="gguf"             # Model type
		local quant="Q4_K_M"          # Quantization level
		local repo_name="${SERIES}-${size}-${TUNING}-$(echo $TYPE | tr '[:lower:]' '[:upper:]')"
		local model_name="${SERIES}-${size}-${TUNING}-${quant}.${TYPE}"
		local prompt=$2
		local interactive=$3
		local verbose=$4

		local args=(
			--hf-repo "${HF_USERNAME}/${repo_name}"
			--hf-file "$model_name"
			--prompt "$prompt"
		)

		# Add conversation flag for interactive mode
		if [[ $interactive == true ]]; then
			args+=(--conversation)
		else
			args+=(--reverse-prompt "$STOP_SYMBOL")
		fi

		# Suppress unnecessary output if verbose mode is disabled
		if [[ $verbose != true ]]; then
			llama-cli "${args[@]}" --no-display-prompt 2>/dev/null
		else
			llama-cli "${args[@]}"
		fi
	}

	# Argument parsing and session type determination
	local read_mode=false
	local read_file=""
	local query=""
	local verbose=false

	# Loop through provided arguments
	while [[ $# -gt 0 ]]; do
		case $1 in
		-h | --help)
			show_help
			return 0
			;;
		--read | -r)
			read_mode=true
			# Check if a file is specified with the read flag
			if [[ -n $2 && ! $2 =~ ^- ]]; then
				read_file=$2
				shift
			else
				echo "Error: --read or -r flag requires a file to be specified." >&2
				return 1
			fi
			;;
		-v | --verbose)
			verbose=true
			;;
		*)
			# Concatenate query arguments
			query+="$1 "
			;;
		esac
		shift
	done

	# Handle file content extraction if read mode is enabled
	local content=""
	if [[ $read_mode == true ]]; then
		if [[ -z "$read_file" ]]; then
			echo "Error: No file specified for --read or -r flag." >&2
			return 1
		fi

		content=$(extract_content "$read_file" 10000)

		if [[ $? -ne 0 ]]; then
			echo "Failed to extract content. Exiting."
			return 1
		fi
	fi

	# Execute the appropriate session type based on the query
	if [[ -n "$query" ]]; then
		# One-off query mode
		local one_off_prompt
		one_off_prompt=$(generate_one_off_prompt "$query" "$content" "$read_file")
		start_llama_session "7B" "$one_off_prompt" false $verbose
	else
		# Interactive chat session
		local interactive_prompt
		interactive_prompt=$(generate_interactive_prompt "$content" "$read_file")
		start_llama_session "7B" "$interactive_prompt" true $verbose
	fi
}
