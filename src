#!/usr/bin/env zsh

function yo() {

  if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
    echo "
yo - A command line AI assistant.

Usage:
  yo [question]

  If a question is provided, Yo will answer the question. Otherwise, Yo will enter an interactive session.

Options:
  -h, --help    Show this help message and exit.
"
    return 0
  fi

  # Set the stop character for one-off questions.
  STOP_CHARACTER="✌️"

  # The Hugging Face username
  HF_USERNAME="bartowski"

  # The series, size, tuning, quantization, and type of the model
  SERIES="Qwen2.5-Coder"
  TUNING="Instruct"
  TYPE="gguf"

  ONE_OFF_LLM_PROMPT="
You are playing the role of Yo.
Yo is a helpful assistant that interacts with a users in the terminal through one-off answers.
You are on a MacOS system.
You responses will be shown as raw text.
Your responses should be as short as possible but still provide complete solutions.
End your message with $STOP_CHARACTER to return control to the user.
User Query:$*
Your Short Answer:
"

  INTERACTIVE_LLM_PROMPT="
You are playing the role of Yo.
Yo is a helpful assistant that interacts with a user through an interactive chat interface in the terminal.
The current time and date is: $(date).

Here is some more information about the system you inhabit.
$(system_profiler SPSoftwareDataType SPHardwareDataType | grep -E 'System Version|Model Identifier|Total Number of Cores|Chip|Memory' | grep -v 'Secure Virtual memory')

The user is currently located in the directory: $(pwd).

Here is a tree of the current directory (up to 2 levels deep):
$(ls -alRh)

Here are the 10 most recent shell commands executed by the user:
$(history -10)

Your task is to answer the user's questions as efficiently and as accurately as possible.
You responses will be shown as raw text.
You can ask the user to take debugging or troubleshooting steps to help refine a solution.
Your responses should be as short as possible but still provide complete solutions.
"

  # Run different versions depending on whether or not there's an argument
  if [ -n "$1" ]; then
    # The repository name and model name
    QUANT="Q4_K_M"
    SIZE="3B"
    REPO_NAME="${SERIES}-${SIZE}-${TUNING}-$(echo $TYPE | tr '[:lower:]' '[:upper:]')"
    MODEL_NAME="${SERIES}-${SIZE}-${TUNING}-${QUANT}.${TYPE}"

    llama-cli \
      --hf-repo "${HF_USERNAME}/${REPO_NAME}" \
      --hf-file $MODEL_NAME \
      --prompt "$ONE_OFF_LLM_PROMPT" \
      --reverse-prompt $STOP_CHARACTER \
      --no-display-prompt \
      2>/dev/null

  else
    # The repository name and model name

    QUANT="Q4_K_M"
    SIZE="7B"
    REPO_NAME="${SERIES}-${SIZE}-${TUNING}-$(echo $TYPE | tr '[:lower:]' '[:upper:]')"
    MODEL_NAME="${SERIES}-${SIZE}-${TUNING}-${QUANT}.${TYPE}"

    llama-cli \
      --hf-repo "${HF_USERNAME}/${REPO_NAME}" \
      --hf-file $MODEL_NAME \
      --prompt "$INTERACTIVE_LLM_PROMPT" \
      --conversation \
      --no-display-prompt \
      2>/dev/null
  fi
}
