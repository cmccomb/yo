#!/usr/bin/env zsh

# `yo` - A command-line AI assistant
function yo() {

	# Version
	local VERSION="0.1.0"

	# Symbol used to signal the end of one-off sessions
	local STOP_SYMBOL="✌️"

	# Display version information
	function show_version() {
		echo "yo v$VERSION"
	}

	# Display usage instructions
	function show_help() {
		cat <<-EOF
			yo - A command-line AI assistant.

			Usage:
			  yo [options] [question]

			Description:
			  If a question is provided, Yo will answer the question. Otherwise, Yo will enter an interactive session.

			Options:
			  -h, --help              Show this help message and exit.
			  -r, --read [file|url]   Integrate information from the specified file (supports plain text files, URLs, and PDFs).
			  -s, --search            Integrate Google search results.
			  -v, --verbose           Enable verbose mode for detailed output.
			  -V, --version           Show the version and exit.

      Examples:
        1. Answer a question:
          $ yo "What is the capital of France?"
        2. Start an interactive session:
          $ yo
        3. Integrate information from a file:
          $ yo --read src "How can I improve this source code?"
        4. Integrate information from a URL:
          $ yo --read https://www.wikipedia.org/wiki/Paris how big is paris
        5. Integrate Google search results:
          $ yo --search what is the capital of tobago

		EOF
	}

	# Generate a prompt for one-off or interactive sessions
	function generate_prompt() {
		local mode=$1 content=$2 filename=$3 query=$4 search_info=$5 search_terms=$6
		local prompt=""
		prompt+="You are playing the role of Yo, a highly-capable AI assistant for the MacOS terminal. "
		prompt+="It is currently $(date). "
		prompt+="You were invoked from the $(pwd) directory "
		prompt+="on a $(system_profiler SPHardwareDataType | grep "Model Name" | awk -F": " '{print $2}') "
		prompt+="with $(sysctl -n hw.ncpu) cores, "
		prompt+="$(sysctl -n hw.memsize | awk '{x=$1/1024/1024/1024; print x}')GB RAM, "
		prompt+="and $(df -h / | tail -1 | awk '{split($4, a, "G"); print a[1]}')GB free disk space.\n\n"

		if [[ -n "$content" ]]; then
			prompt+="Relevant Information from file \"$filename\":\n"
			prompt+="================= BEGINNING OF FILE CONTENTS =================\n"
			prompt+="$content\n"
			prompt+="==================== END OF FILE CONTENTS ====================\n\n"
		fi

		if [[ -n "$search_terms" ]]; then
			prompt+="Relevant Information from web search using terms \"$search_terms\":\n"
			prompt+="================= BEGINNING OF SEARCH RESULTS =================\n"
			prompt+="$search_info\n"
			prompt+="==================== END OF SEARCH RESULTS ====================\n\n"
		fi

		if [[ $mode == "one-off" ]]; then
			prompt+="Your task is to answer the user's question directly. "
			prompt+="Your answer will be concise, helpful, and actionable. "
			prompt+="End your answer with the stop symbol '$STOP_SYMBOL'.\n\n"
			prompt+="User Query: $query $STOP_SYMBOL\n"
			prompt+="Your Super-Short Answer:"
		else
			prompt+="Your task is to assist the user in an interactive session, responding concisely and accurately."
		fi

		echo "$prompt"
	}

	# Extract content from a file or URL (supports text and PDF files)
	function extract_content() {
		local source=$1 max_length=$2
		local content

		if [[ $source =~ ^https?:// ]]; then
			# Fetch content from URL
			content=$(curl -s "$source" | pandoc -f html -t plain --quiet)
			if [[ $? -ne 0 ]]; then
				echo "Error: Failed to fetch content from URL." >&2
				return 1
			fi
		else
			# Fetch content from file
			[[ ! -f "$source" ]] && {
				echo "Error: File not found." >&2
				return 1
			}

			case $source in
			*.pdf)
				command -v pdftotext >/dev/null 2>&1 || {
					echo "Error: pdftotext not installed. Install it using your package manager (e.g., brew install poppler)." >&2
					return 1
				}
				content=$(pdftotext "$source" - 2>/dev/null)
				;;
			*.txt | *)
				content=$(cat "$source")
				;;
			esac
		fi

		[[ -n "$max_length" && "$max_length" -gt 0 ]] && content=${content:0:$max_length}
		echo "$content"
	}

	# Perform a web search and extract plain text results
	function perform_search() {
		local url result search_term_slug
		search_term_slug=$(echo $1 | sed 's/ /+/g')
		url="https://www.google.com/search?q=$search_term_slug"
		result=$(curl -sL $url | pandoc -f html -t plain --quiet)
		if [[ $? -ne 0 ]]; then
			echo "Error: Failed to perform web search." >&2
			return 1
		fi

		# Remove everything after "People also search for"
		result=$(echo "$result" | sed -n '/People also search for/q;p')

		echo "$result"
	}

	# Extract optimized search terms using the 3B LLM
	function extract_search_terms() {
		local query=$1 verbose=$2
		local prompt="Your task is to extract the most relevant search terms from the following query for a web search.

    Here is an example:
    User Query: how large is the capital of france $STOP_SYMBOL
    Search Terms: paris capital population area

    Here is another:
    User Query: what is the furthest planet from the sun $STOP_SYMBOL
    Search Terms: solar system furthest planet distance

    Here is the real one!
    User Query: $query
    Search Terms: "

		# Generate
		local terms
		if [[ $verbose != true ]]; then
			terms=$(llama-cli --threads "$(sysctl -n hw.logicalcpu_max)" --hf-repo "bartowski/Qwen2.5-1.5B-Instruct-GGUF" --hf-file "Qwen2.5-1.5B-Instruct-Q4_K_M.gguf" --prompt-cache "/tmp/.yo_search_terms" -n 8 --no-warmup --prompt "$prompt" --temp 0.2 --seed 42 --flash-attn --prio 3 --mirostat 2 --no-display-prompt 2>/dev/null)
		else
			terms=$(llama-cli --threads "$(sysctl -n hw.logicalcpu_max)" --hf-repo "bartowski/Qwen2.5-1.5B-Instruct-GGUF" --hf-file "Qwen2.5-1.5B-Instruct-Q4_K_M.gguf" --prompt-cache "/tmp/.yo_search_terms" -n 8 --no-warmup --prompt "$prompt" --temp 0.2 --seed 42 --flash-attn --prio 3 --mirostat 2 --no-display-prompt --verbose-prompt)
		fi

		# Only take the first line
		terms=$(echo "$terms" | head -n 1)

		# Remove end of text if needed
		terms=$(echo "$terms" | sed 's/\[end of text\]//g')

		# Return results
		echo "$terms"
	}

	# Start a llama-cli session
	function start_llama_session() {
		local repo_name=$1 model_name=$2 prompt=$3 interactive=$4 verbose=$5
		local args=(
			--threads "$(sysctl -n hw.logicalcpu_max)"
			--hf-repo "$repo_name"
			--hf-file "$model_name"
			--temp 0.2
			--prompt "$prompt"
			--seed 42
			--flash-attn
			--prio 3
			--mirostat 2
		)

		if [[ $interactive == true ]]; then
			args+=(--conversation --prompt-cache "/tmp/.yo_interactive_prompt_cache_$repo_name_$model_name")
		else
			args+=(--reverse-prompt "$STOP_SYMBOL" --prompt-cache "/tmp/.yo_oneoff_prompt_cache_$repo_name_$model_name")
		fi

		if [[ $verbose != true ]]; then
			llama-cli "${args[@]}" --no-display-prompt 2>/dev/null
		else
			llama-cli "${args[@]}" --verbose-prompt
		fi
	}

	# Parse arguments
	local read_mode=false read_file="" query="" verbose=false search_mode=false
	while [[ $# -gt 0 ]]; do
		case $1 in
		-h | --help)
			show_help
			return 0
			;;
		-r | --read)
			read_mode=true
			[[ -n $2 && ! $2 =~ ^- ]] && {
				read_file=$2
				shift
			} || {
				echo "Error: --read requires a file." >&2
				return 1
			}
			;;
		-v | --verbose) verbose=true ;;
		-V | --version)
			show_version
			return 0
			;;
		-s | --search) search_mode=true ;;
		*) query+="$1 " ;;
		esac
		shift
	done

	# Handle file reading
	local content=""
	if [[ $read_mode == true && -n "$read_file" ]]; then
		content=$(extract_content "$read_file" 10000) || return 1
	fi

	local search_info="" search_terms=""
	if [[ $search_mode == true && -n "$query" ]]; then
		search_terms=$(extract_search_terms "$query" $verbose)
		search_info=$(perform_search "$search_terms")
	fi

	local quant_provider="bartowski" # Hugging Face username
	local model_series="Qwen2.5"     # Model series
	local tuning="Instruct"          # Model tuning
	local filetype="gguf"            # Model type

	# Configure model settings
	local quant size repo_name model_name
	if [[ -n "$query" ]]; then
		size="3B"
		quant="Q4_K_M"
	else
		size="14B"
		quant="IQ4_XS"
	fi
	repo_name="${quant_provider}/${model_series}-${size}-${tuning}-$(echo $filetype | tr '[:lower:]' '[:upper:]')"
	model_name="${model_series}-${size}-${tuning}-${quant}.${filetype}"

	# Generate and start appropriate session
	local prompt
	if [[ -n "$query" ]]; then
		prompt=$(generate_prompt "one-off" "$content" "$read_file" "$query" "$search_info" "$search_terms")
		start_llama_session "$repo_name" "$model_name" "$prompt" false $verbose
	else
		prompt=$(generate_prompt "interactive" "$content" "$read_file" "" "$search_info" "$search_terms")
		start_llama_session "$repo_name" "$model_name" "$prompt" true $verbose
	fi
}
