#!/usr/bin/env zsh

########################################################################################################################
########################################################################################################################
######'                     `########'                ╭─────────────────────────────────────────╮                 `#####
####'    db    db  .d88b.     `####'                  │  Your AI Assistant in the Command Line  │                  `####
####     `8b  d8' .8P  Y8.     ####                   ╰─────────────────────────────────────────╯                   ####
####      `8bd8'  88    88     ####     If you are here, you are probably trying to fix or change something. If     ####
####        88    88    88     ####     you are here to fix something,  I am sorry it broke in the first place!     ####
####        88    `8b  d8'     ####     But either way, I hope that the documentation is helpful and I wish you     ####
####.       YP     `Y88P'     .####.     the best of luck. If you need help, please email: ccmcc2012@gmail.com     .####
######.                     .########.                                                                           .######
########################################################################################################################
########################################################################################################################

function yo() {

	######################################################################################################################
	### THE YO ###########################################################################################################
	######################################################################################################################

	local YO="✌️"
	readonly YO

	######################################################################################################################
	### THE VERSION ######################################################################################################
	######################################################################################################################

	### Define the version of the script #################################################################################

	### All versioning follows semver as defined at https://semver.org/ ##################################################
	local VERSION="0.1.0"
	readonly VERSION

	### Display version information ######################################################################################
	function show_version() {
		echo "yo v${VERSION} ${YO}"
	}

	######################################################################################################################
	### CONSTANTS AND SETTINGS ##########################################################################################
	######################################################################################################################

	local GENERAL_USERNAME="bartowski"
	local GENERAL_FINETUNING_STYLE="Instruct"
	local GENERAL_MODEL_FILETYPE="gguf"

	### Define parameters for the task model #############################################################################

	# Model parameters
	local TASK_USERNAME=$GENERAL_USERNAME
	local TASK_MODEL_SERIES="Llama-3.2"
	local TASK_MODEL_FINETUNING_STYLE=$GENERAL_FINETUNING_STYLE
	local TASK_MODEL_SIZE="1B"
	local TASK_MODEL_QUANT="Q4_K_M"
	local TASK_MODEL_FILETYPE=$GENERAL_MODEL_FILETYPE

	# Generation parameters
	local TASK_MODEL_NEW_TOKENS=16
	local TASK_MODEL_TEMP=0.2

	### Define parameters for the casual model ###########################################################################

	# Model parameters
	local CASUAL_GENERAL_USERNAME=$GENERAL_USERNAME
	local CASUAL_MODEL_SERIES="Qwen2.5"
	local CASUAL_MODEL_FINETUNING_STYLE=$GENERAL_FINETUNING_STYLE
	local CASUAL_MODEL_SIZE="3B"
	local CASUAL_MODEL_QUANT="Q4_K_M"
	local CASUAL_MODEL_FILETYPE=$GENERAL_MODEL_FILETYPE

	# Generation parameters
	local CASUAL_MODEL_NEW_TOKENS=128
	local CASUAL_MODEL_TEMP=0.2

	### Define parameters for the balanced model #########################################################################
	local BALANCED_GENERAL_USERNAME=$GENERAL_USERNAME
	local BALANCED_MODEL_SERIES="Qwen2.5"
	local BALANCED_MODEL_FINETUNING_STYLE=$GENERAL_FINETUNING_STYLE
	local BALANCED_MODEL_SIZE="7B"
	local BALANCED_MODEL_QUANT="Q4_K_M"
	local BALANCED_MODEL_FILETYPE=$GENERAL_MODEL_FILETYPE

	# Generation parameters
	local BALANCED_MODEL_TEMP=0.2

	### Define the series model parameters ###############################################################################
	local SERIOUS_GENERAL_USERNAME=$GENERAL_USERNAME
	local SERIOUS_MODEL_SERIES="Qwen2.5-Coder"
	local SERIOUS_GENERAL_FINETUNING_STYLE=$GENERAL_FINETUNING_STYLE
	local SERIOUS_MODEL_SIZE="14B"
	local SERIOUS_MODEL_QUANT="IQ4_XS"
	local SERIOUS_GENERAL_MODEL_FILETYPE=$GENERAL_MODEL_FILETYPE

	# Generation parameters
	local SERIOUS_MODEL_NEW_TOKENS=512
	local SERIOUS_MODEL_TEMP=0.2

	### Custom search API key ############################################################################################
	local GOOGLE_CSE_API_KEY="AIzaSyBBXNq-DX1ENgFAiGCzTawQtWmRMSbDljk"
	local GOOGLE_CSE_ID="003333935467370160898:f2ntsnftsjy"
	local GOOGLE_CSE_BASE_URL="https://customsearch.googleapis.com/customsearch/v1"
	readonly GOOGLE_CSE_API_KEY GOOGLE_CSE_ID GOOGLE_CSE_BASE_URL

	### Maximum length of file content to extract ########################################################################
	local MAX_FILE_CONTENT_LENGTH=10000

	### Set the repository and file names ################################################################################

	# Define a function to help
	function compose_repo_and_model_FILE_name() {
		local username=$1 series=$2 size=$3 finetuning=$4 filetype=$5 quant=$6
		echo "${username}/${series}-${size}-${finetuning}-$(echo "${filetype}" |
			tr '[:lower:]' '[:upper:]') ${repo_name} ${series}-${size}-${finetuning}-${quant}.${filetype}"
	}

	# Set the repository and model names for the task model
	local TASK_MODEL_REPO_NAME TASK_MODEL_FILE_NAME
	read -r TASK_MODEL_REPO_NAME TASK_MODEL_FILE_NAME <<<"$(
		compose_repo_and_model_FILE_name \
			${TASK_USERNAME} \
			${TASK_MODEL_SERIES} \
			${TASK_MODEL_SIZE} \
			${TASK_MODEL_FINETUNING_STYLE} \
			${TASK_MODEL_FILETYPE} \
			${TASK_MODEL_QUANT}
	)"
	readonly TASK_MODEL_REPO_NAME TASK_MODEL_FILE_NAME

	# Set the repository and model names for the casual model
	local CASUAL_MODEL_FILE_NAME CASUAL_MODEL_REPO_NAME
	read -r CASUAL_MODEL_REPO_NAME CASUAL_MODEL_FILE_NAME <<<"$(
		compose_repo_and_model_FILE_name \
			${CASUAL_GENERAL_USERNAME} \
			${CASUAL_MODEL_SERIES} \
			${CASUAL_MODEL_SIZE} \
			${CASUAL_MODEL_FINETUNING_STYLE} \
			${CASUAL_MODEL_FILETYPE} \
			${CASUAL_MODEL_QUANT}
	)"
	readonly CASUAL_MODEL_REPO_NAME CASUAL_MODEL_FILE_NAME

	# Set the repository and model names for the balanced model
	local BALANCED_MODEL_FILE_NAME BALANCED_MODEL_REPO_NAME
	read -r BALANCED_MODEL_REPO_NAME BALANCED_MODEL_FILE_NAME <<<"$(
		compose_repo_and_model_FILE_name \
			${BALANCED_GENERAL_USERNAME} \
			${BALANCED_MODEL_SERIES} \
			${BALANCED_MODEL_SIZE} \
			${BALANCED_MODEL_FINETUNING_STYLE} \
			${BALANCED_MODEL_FILETYPE} \
			${BALANCED_MODEL_QUANT}
	)"
	readonly BALANCED_MODEL_REPO_NAME BALANCED_MODEL_FILE_NAME

	# Set the repository and model names for the serious model
	local SERIOUS_MODEL_FILE_NAME SERIOUS_MODEL_REPO_NAME
	read -r SERIOUS_MODEL_REPO_NAME SERIOUS_MODEL_FILE_NAME <<<"$(
		compose_repo_and_model_FILE_name \
			${SERIOUS_GENERAL_USERNAME} \
			${SERIOUS_MODEL_SERIES} \
			${SERIOUS_MODEL_SIZE} \
			${SERIOUS_GENERAL_FINETUNING_STYLE} \
			${SERIOUS_GENERAL_MODEL_FILETYPE} \
			${SERIOUS_MODEL_QUANT}
	)"
	readonly SERIOUS_MODEL_REPO_NAME SERIOUS_MODEL_FILE_NAME

	######################################################################################################################
	### UTILITY FUNCTIONS ################################################################################################
	######################################################################################################################

	### Display usage instructions #######################################################################################
	function show_help() {
		cat <<-EOF
			yo - A command-line AI assistant.

			Usage:
			  yo [options] [question]

			Description:
			  If a question is provided, Yo will answer the question. Otherwise, Yo will enter an interactive session.

			Context Options:
			  These options allow you to control the information that Yo uses to answer your question.

			  First, we have local options - the options that don't require an internet connection:
			    -c, --clipboard         Copy the contents of the clipboard into Yo's context.
			    -d, --directory         Include a list of the files in the current directory in Yo's context.
			    -r, --read "PATH"       Extract the specified file or URL and into Yo's context. Supports text-based files
			                            (e.g., .txt, .md, .py), PDF files via pdftotext, and web pages via curl and pandoc.
			    -y, --system            Run a few system commands and integrate the information into Yo's context.

			  Next, we have several option that require an internet connection:
			    -s, --search "TERMS"    Perform a web search using the specified quoted terms and integrate the results into
			                            Yo's context. Requires an active internet connection.
			    -S, --surf              Perform a web search using LLM-chosen terms based on the question. Integrates results
			                            into Yo's context. Requires an active internet connection.

			Model Size Options:
			  Yo uses four distinct models: a task model (${TASK_MODEL_FILE_NAME}), a casual model (${CASUAL_MODEL_FILE_NAME}),
			  a balanced model (${BALANCED_MODEL_FILE_NAME}), and a serious model (${SERIOUS_MODEL_FILE_NAME}). By default,
			  Yo uses the task model for summarization and other small tasks, the casual model for one-off questions, and the
			  serious model for interactive sessions. You can override the default model by using the following options:
			    -tm, --task-model       Use the task model
			    -cm, --casual-model     Use the casual model
			    -bm, --balanced-model   Use the balanced model
			    -sm, --serious-model    Use the serious model

			General Purpose Options:
			  These options provide general functionality.
			    -h, --help              Show this help message and exit.
			    -v, --verbose           Enable verbose mode for detailed output.
			    -V, --version           Show the version and exit.

			Examples:
			  * Answer a question:
			    $ yo "What is the capital of France?"

			  * For simple cases, you can omit the quotes:
			    $ yo what is the capital of france

			  * Start an interactive session:
			    $ yo

			  * Integrate information from a file:
			    $ yo --read src "How can I improve this source code?"

			  * Integrate information from a URL:
			    $ yo --read https://en.wikipedia.org/wiki/Paris "how big is paris"

			  * Integrate Google search results:
			    $ yo --search "what is the capital of Tobago"

			  * Use LLM-selected search terms:
			    $ yo --surf tell me about renewable energy trends.

			  * Combine context sources:
			    $ yo --read src --search "what is the capital of france"

			  * Add verbosity to any of these commands
			    $ yo what is the capital of france --verbose
		EOF
		return 0
	}

	# Check if the input is empty
	function check_nonempty() {

		# Parse arguments
		local variable_name=$1
		local variable_value=$2

		# Check if the input is empty
		if [[ -z "${variable_value}" ]]; then
			echo "Error: Invalid input ${variable_name}=\"${variable_value}\", expected a non-empty string." >&2
			return 1
		else
			return 0
		fi
	}

	# Check if the input is an integer and print an error message with the name of the variable if not
	function check_integer() {

		# Parse arguments
		local variable_name=$1
		local variable_value=$2

		# Check that inputs are non-empty
		check_nonempty "variable_name" "${variable_name}" || return 1
		check_nonempty "variable_value" "${variable_value}" || return 1

		# Check if the input is an integer
		if [[ ! "${variable_value}" =~ ^[0-9]+$ ]]; then
			echo "Error in ${funcstack[2]}: Invalid input ${variable_name}=\"${variable_value}\", expected an integer." >&2
			return 1
		else
			return 0
		fi
	}

	# Check if the input is Boolean
	function check_boolean() {

		# Parse arguments
		local variable_name=$1
		local variable_value=$2

		# Check that inputs are non-empty
		check_nonempty "variable_name" "${variable_name}" || return 1
		check_nonempty "variable_value" "${variable_value}" || return 1

		# Check if the input is Boolean
		if [[ "${variable_value}" != true && "${variable_value}" != false ]]; then
			echo "Error in ${funcstack[2]}: Invalid input ${variable_name}=\"${variable_value}\", expected a boolean." >&2
			return 1
		else
			return 0
		fi
	}

	# Check if the input is a float
	function check_float() {

		# Parse arguments
		local variable_name=$1
		local variable_value=$2

		# Check that inputs are non-empty
		check_nonempty "variable_name" "${variable_name}" || return 1
		check_nonempty "variable_value" "${variable_value}" || return 1

		# Check if the input is a float
		if [[ ! $variable_value =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
			echo "Error in ${funcstack[2]}: Invalid input ${variable_name}=\"${variable_value}\", expected a float." >&2
			return 1
		else
			return 0
		fi
	}

	# Function that checks to see if a model exists and download it if not
	function check_model_download() {
		# Parse arguments
		local repo_name=$1 file_name=$2 verbose=$3

		# Check that inputs are valid
		check_nonempty "repo_name" "${repo_name}" || return 1
		check_nonempty "file_name" "${file_name}" || return 1

		# Check if the model exists
		local model_path="/Users/${USER}/Library/Caches/llama.cpp/${repo_name//\//_}_${file_name}"
		if [[ ! -f "${model_path}" ]]; then
			# Print a detailed timestamp, down to the decimal seconds
			echo "[$(gdate "+%H:%M:%S.%2N")] 🌐 Downloading ${repo_name}/${file_name}..." >&2
			if [[ "${verbose}" == true ]]; then
				# Print message about downloading model to stderr
				if ! llama-cli --hf-repo "${repo_name}" --hf-file "${file_name}" -p "hi" -n 0; then
					echo "Error in ${funcstack[3]}: Failed to download ${repo_name}/${file_name}." >&2
					return 1
				fi
			else
				if ! llama-cli --hf-repo "${repo_name}" --hf-file "${file_name}" --no-warmup -p "hi" -n 0 2>/dev/null; then
					echo "Error in ${funcstack[3]}: Failed to download ${repo_name}/${file_name}." >&2
					return 1
				fi
			fi
		fi
	}

	######################################################################################################################
	### SEARCH ###########################################################################################################
	######################################################################################################################

	### Perform a web search with user-provided terms ####################################################################
	function perform_search() {

		# Parse arguments
		local terms=$1

		# Check that inputs are valid
		check_nonempty "terms" "${terms}" || return 1

		# Make variables
		local url response

		# Example API call
		url="${GOOGLE_CSE_BASE_URL}?key=${GOOGLE_CSE_API_KEY}&cx=${GOOGLE_CSE_ID}&q=${terms// /%20}"

		# Perform search and extract relevant information
		if ! response=$(\curl -s "${url}" | grep -E "^      \"title\"|^      \"snippet\""); then
			echo "Error: Failed to perform web search." >&2
			return 1
		else
			# Return response
			echo "${response}"
			return 0
		fi
	}

	### Extract optimized search terms using a small model ###############################################################
	function extract_search_terms() {

		# Parse arguments
		local query=$1 verbose=$2

		# Check that inputs are valid
		check_nonempty "query" "${query}" || return 1
		check_boolean "verbose" "${verbose}" || return 1

		# Make variables
		local prompt="" terms

		# Generate prompt
		prompt+="Your task is to extract the most relevant search terms from the following query for a web search.\n\n"
		prompt+="Here is an example:\n"
		prompt+="User Query: how large is the capital of france ${YO}\n"
		prompt+="Search Terms: paris capital population area\n\n"
		prompt+="Here is another example:\n"
		prompt+="User Query: what is the furthest planet from the sun ${YO}\n"
		prompt+="Search Terms: solar system furthest planet distance\n\n"
		prompt+="Here is the real user query.\n"
		prompt+="User Query: ${query}\n"
		prompt+="Search Terms: "

		# Generate response
		terms=$(
			start_llama_session \
				"${TASK_MODEL_REPO_NAME}" \
				"${TASK_MODEL_FILE_NAME}" \
				"${prompt}" \
				false \
				"${verbose}" \
				false \
				"${TASK_MODEL_NEW_TOKENS}" \
				"${TASK_MODEL_TEMP}"
		)

		# Only take the first line
		terms=$(echo "${terms}" | head -n 1)

		# Remove [end of text] marker if needed
		terms="${terms//\[end of text\]/}"

		# Return results
		echo "${terms}"
		return 0
	}

	######################################################################################################################
	### CONTENT EXTRACTION ###############################################################################################
	######################################################################################################################

	### Extract file_info from a file or URL (supports text and PDF files) ###############################################
	function extract_file_info() {

		# Parse arguments
		local source=$1 max_length=$2

		# Check that inputs are valid
		check_nonempty "source" "${source}" || return 1
		check_integer "max_length" "${max_length}" || return 1

		# Make variables
		local file_info=""

		if [[ "${source}" =~ ^https?:// ]]; then
			# Fetch file_info from URL
			if ! file_info=$(curl -s "${source}" | pandoc -f html -t plain --quiet); then
				echo "Error: Failed to fetch file_info from URL." >&2
				return 1
			fi
		else
			# Fetch file_info from file
			[[ ! -f "${source}" ]] && {
				echo "Error: File not found." >&2
				return 1
			}

			case $source in
			*.pdf)
				command -v pdftotext >/dev/null 2>&1 || {
					echo "Error: pdftotext not installed. Install it using your package manager (e.g., brew install poppler)." >&2
					return 1
				}
				file_info=$(pdftotext "${source}" - 2>/dev/null)
				;;
			*.txt | *)
				file_info=$(cat "${source}")
				;;
			esac
		fi

		# Trim to max length if needed
		[[ -n "${max_length}" && "${max_length}" -gt 0 ]] && file_info=${file_info:0:$max_length}

		# Return file_info
		echo "${file_info}"
		return 0
	}

	######################################################################################################################
	### LLMs AND PROMPTS #################################################################################################
	######################################################################################################################

	### Start by establishing some prompt generators #####################################################################

	# Generate base prompt
	function generate_base_prompt() {
		cat <<-EOF
			You are playing the role of Yo, a highly-capable AI assistant living in the MacOS terminal. It is currently $(date).
		EOF
	}

	# Generate system information
	function generate_system_info_context() {

		# Make variables
		local model cores ram free_storage

		# Get system information
		model=$(system_profiler SPHardwareDataType | grep "Model Name" | awk -F": " '{print $2}')
		cores=$(sysctl -n hw.ncpu)
		ram=$(sysctl -n hw.memsize | awk '{x=$1/1024/1024/1024; print x}')
		free_storage=$(df -h / | tail -1 | awk '{split($4, a, "G"); print a[1]}')

		# Return system information
		cat <<-EOF
			===================== BEGINNING OF SYSTEM INFORMATION =====================
			You are on a ${model} with ${cores} cores, ${ram}GB RAM, and ${free_storage}GB free disk space.
			======================== END OF  SYSTEM INFORMATION =======================
		EOF
	}

	# Generate directory information
	function generate_directory_info_context() {

		# Make variables
		local file_list file_previews

		# Generate file list and previews
		file_list=$(ls -lahpSR | head -n 50)
		file_previews=$(
			find . -maxdepth 1 -type f -exec file --mime {} + |
				grep 'text/' |
				cut -d: -f1 |
				xargs du -h |
				sort -rh |
				head -n 5 |
				awk '{print $2}' |
				xargs -I {} sh -c 'echo "\n\nFile: {}"; echo ---\\n$(head -n 10 "{}")\\n---'
		)

		cat <<-EOF
			================= BEGINNING OF CURRENT DIRECTORY CONTENTS =================
			You were invoked from the $(pwd) directory.

			Here are the contents (truncated at 50, sorted by file size):
			${file_list}

			Here is a preview of the contents of the largest readable files:
			${file_previews}
			==================== END OF CURRENT DIRECTORY CONTENTS ====================
		EOF

	}

	# Generate clipboard information
	function generate_clipboard_info_context() {

		cat <<-EOF
			Here are the contents from the clipboard:
			================= BEGINNING OF CURRENT CLIPBOARD CONTENTS =================
			$(pbpaste)
			==================== END OF CURRENT CLIPBOARD CONTENTS ====================
		EOF
	}

	# Generate file contents context
	function generate_file_context() {

		# Parse arguments
		local file=$1 file_info=$2

		# Return file information
		cat <<-EOF
			Relevant information from ${file}:
			================= BEGINNING OF FILE CONTENTS =================
			${file_info}
			===================== END OF FILE CONTENTS ===================
		EOF
	}

	# Generate search context
	function generate_search_context() {

		# Parse arguments
		local search_terms=$1 search_info=$2

		# Return search information
		cat <<-EOF
			Relevant information from web search using ${search_terms}:
			================= BEGINNING OF SEARCH RESULTS =================
			${search_info}
			===================== END OF SEARCH RESULTS ====================
		EOF
	}

	# Generate prompt for one-off sessions
	function generate_oneoff_instructions() {

		# Parse arguments
		local query=$1

		cat <<-EOF
			Your task is to directly answer the user's question. Your answer will be concise, helpful, and immediately usable. End your answer with the symbol ${YO}.

			Here is an example:
			User Query:how large is the capital of france ${YO}
			Your Super-Short Answer:41 square miles (105 square km) ${YO}

			Here is another example:
			User Query:what is the furthest planet from the sun ${YO}
			Search Terms:Neptune is the furthest planet from the Sun. ${YO}

			Here is the real user query.
			User Query:${query} ${YO}
			Your Super-Short Answer:
		EOF
	}

	# Generate interactive instructions
	function generate_interactive_instructions() {

		cat <<-EOF
			Your task is to assist the user in an interactive session, responding concisely and accurately.
		EOF
	}

	### Generate a prompt for one-off or interactive sessions ############################################################
	function generate_prompt() {

		# Parse arguments
		local interactive=$1 file_info=$2 filename=$3 query=$4 search_info=$5 search_terms=$6 add_system_info=$7 \
			add_directory_info=$8 add_clipboard_info=$9

		# Check that inputs are valid
		check_boolean "interactive" "${interactive}" || return 1
		check_boolean "add_system_info" "${add_system_info}" || return 1
		check_boolean "add_directory_info" "${add_directory_info}" || return 1
		check_boolean "add_clipboard_info" "${add_clipboard_info}" || return 1

		# Make the base prompt
		local prompt
		prompt=$(generate_base_prompt)

		# Add system information if requested
		if [[ "$add_system_info" == true ]]; then
			echo "[$(gdate "+%H:%M:%S.%2N")] 💻 Querying system info..." >&2
			prompt+=$(generate_system_info_context)+"\n\n"
		fi

		# Add directory information if requested
		if [[ "${add_directory_info}" == true ]]; then
			echo "[$(gdate "+%H:%M:%S.%2N")] 📂 Scraping the local directory..." >&2
			prompt+=$(generate_directory_info_context)+"\n\n"
		fi

		# Add clipboard information if requested
		if [[ "${add_clipboard_info}" == true ]]; then
			echo "[$(gdate "+%H:%M:%S.%2N")] 📋 Checking out the clipboard..." >&2
			prompt+=$(generate_clipboard_info_context)+"\n\n"
		fi

		# Add file file_info if available
		if [[ -n "${file_info}" ]]; then
			echo "[$(gdate "+%H:%M:%S.%2N")] 📚 Reviewing ${filename}..." >&2
			prompt+=$(generate_file_context "${filename}" "${file_info}")+"\n\n"
		fi

		# Add search information if available
		if [[ -n "${search_terms}" ]]; then
			echo "[$(gdate "+%H:%M:%S.%2N")] 🔎 Searching for ${search_terms}..." >&2
			prompt+=$(generate_search_context "${search_terms}" "${search_info}")+"\n\n"
		fi

		# Add query and instructions based on interactive ##################################################################
		if [[ "${interactive}" == false ]]; then
			prompt+=$(generate_oneoff_instructions "${query}")
		else
			prompt+=$(generate_interactive_instructions)
		fi

		# Return prompt
		echo "${prompt}"
		return 0
	}

	### Start a llama-cli session ########################################################################################
	function start_llama_session() {

		# Parse arguments
		local repo_name=$1 file_name=$2 prompt=$3 interactive=$4 squash_stderr=$5 display_prompt=$6 \
			number_of_tokens_to_generate=$7 temp=$8

		# Check that inputs are valid
		check_nonempty "repo_name" "${repo_name}" || return 1
		check_nonempty "file_name" "${file_name}" || return 1
		check_nonempty "prompt" "${prompt}" || return 1
		check_boolean "interactive" "${interactive}" || return 1
		check_boolean "squash_stderr" "${squash_stderr}" || return 1
		check_boolean "display_prompt" "${display_prompt}" || return 1
		check_integer "number_of_tokens_to_generate" "${number_of_tokens_to_generate}" || return 1
		check_float "temp" "${temp}" || return 1

		# Check if the model exists and download it if not
		check_model_download "${repo_name}" "${file_name}" "${squash_stderr}" || return 1

		# Configure llama-cli arguments
		local args=(
			--threads "$(sysctl -n hw.logicalcpu_max)"
			--hf-repo "$repo_name"
			--hf-file "$file_name"
			--prompt "$prompt"
			--prompt-cache "/tmp/yo_prompt_cache_${file_name}"
			--predict "${number_of_tokens_to_generate}"
			--temp "${temp}"
			--seed 42
			--prio 3
			--mirostat 2
			--flash-attn
			--no-warmup
		)

		# Add conversation or reverse-prompt based on mode
		if [[ "${interactive}" == true ]]; then
			echo "[$(gdate "+%H:%M:%S.%2N")] 💭 Getting ready for our conversation..." >&2
			args+=(--conversation)
		else
			echo "[$(gdate "+%H:%M:%S.%2N")] 💭 Thinking about the question..." >&2 # TODO This is triggered for the task model but shouldn't be
			args+=(--reverse-prompt "${YO}")
		fi

		# Display prompt
		if [[ "${display_prompt}" != true ]]; then
			args+=(--no-display-prompt)
		else
			args+=(--verbose-prompt)
		fi

		# Start session
		if [[ "${squash_stderr}" != true ]]; then
			if ! llama-cli "${args[@]}" 2>/dev/null; then
				echo "Error: llama-cli command failed while attempting to call ${repo_name}/${file_name}." >&2
				return 1
			fi
		else
			if ! llama-cli "${args[@]}"; then
				echo "Error: llama-cli command failed while attempting to call ${repo_name}/${file_name}." >&2
				return 1
			fi
		fi
	}

	######################################################################################################################
	### MAIN FUNCTION ####################################################################################################
	######################################################################################################################

	### Parse arguments ##################################################################################################

	# Define variables
	local verbose=false query="" # TODO make verbose take on multiple possible values
	local read_mode=false read_file="" file_info=""
	local surf_mode=false search_mode=false search_terms="" search_info=""
	local add_directory_info=false add_system_info=false add_clipboard_info=false
	local task_model_override=false casual_model_override=false balanced_model_override=false
	local serious_model_override=false
	local quant size repo_name file_name new_tokens temp prompt

	# Start parsing arguments
	while [[ $# -gt 0 ]]; do
		case $1 in

		# Early exit with help message
		-h | --help)
			show_help
			return 0
			;;

		# Early exit with version information
		-V | --version)
			show_version
			return 0
			;;

		# Read in a file
		-r | --read)
			read_mode=true
			if [[ -n $2 && ! $2 =~ ^- ]]; then
				read_file=$2 # TODO make this add files to a list
				shift # TODO distinguish explicitly between local reading and web reading
			else
				echo "Error: --read requires a file." >&2
				return 1
			fi
			;;

		# Do some searching
		-s | --search)
			search_mode=true
			if [[ -n $2 && $2 =~ ^".*"$ ]]; then
				search_terms=$2
				shift
			else
				echo "Error: --search requires quoted terms." >&2
				return 1
			fi
			;;

		# Make the output verbose
		-v | --verbose) verbose=true ;;

		# Surf the web with LLM-defined search terms
		-S | --surf) surf_mode=true ;;

		# Add system information to the context
		-y | --system) add_system_info=true ;;

		# Add directory information to the context
		-d | --directory) add_directory_info=true ;;

		# Add clipboard information to the context
		-c | --clipboard) add_clipboard_info=true ;;

		# Use the task model
		-tm | --task-model) task_model_override=true ;;

		# Use the casual model
		-cm | --casual-model) casual_model_override=true ;;

		# Use the balanced model
		-bm | --balanced-model) balanced_model_override=true ;;

		# Use the serious model
		-sm | --serious-model) serious_model_override=true ;;

		# If its something that looks like a flag but isn't one of the above, show an error
		-*)
			echo "Error: Unknown flag: $1" >&2
			return 1
			;;

		# If its not a flag, add it to the general query
		*) query+="$1 " ;;
		esac
		shift
	done

	### Print the starting time ###########################################################################################

	# Print a detailed timestamp, down to the decimal seconds
	echo "[$(gdate "+%H:%M:%S.%2N")] ⏳️ Starting..." >&2

	# Save starting time to calculate elapsed time later one
	local start_time
	start_time=$(gdate +%s.%N)

	### Read in files if needed ##########################################################################################
	if [[ "${read_mode}" == true && -n "${read_file}" ]]; then
		echo "Reading in file..." >&2 # TODO move this inside the context generator
		file_info=$(extract_file_info "${read_file}" "${MAX_FILE_CONTENT_LENGTH}") || return 1
	fi

	### Bring in search information if needed ############################################################################
	if [[ "${search_mode}" == true ]]; then                   # TODO move this inside the context generator
		echo "Performing search with user-supplied terms..." >&2 # TODO separate this out so we can have specific contexts AND surfing
		search_info=$(perform_search "${search_terms}")
	elif [[ "${surf_mode}" == true ]]; then
		echo "Performing search with LLM-chosen terms..." >&2
		search_terms=$(extract_search_terms "${query}" "${verbose}")
		search_info=$(perform_search "${search_terms}")
	fi

	### Configure the model based on whether its a one-off or interactive session ########################################
	if [[ -n "${query}" ]]; then
		repo_name="${CASUAL_MODEL_REPO_NAME}"
		file_name="${CASUAL_MODEL_FILE_NAME}"
		new_tokens="${CASUAL_MODEL_NEW_TOKENS}"
		temp="${CASUAL_MODEL_TEMP}"
		interactive=false
	else
		repo_name="${SERIOUS_MODEL_REPO_NAME}"
		file_name="${SERIOUS_MODEL_FILE_NAME}"
		new_tokens="${SERIOUS_MODEL_NEW_TOKENS}"
		temp=${SERIOUS_MODEL_TEMP}
		interactive=true
	fi

	### Override the model if needed ######################################################################################
	if [[ "${task_model_override}" == true ]]; then
		repo_name="${TASK_MODEL_REPO_NAME}"
		file_name="${TASK_MODEL_FILE_NAME}"
		temp="${TASK_MODEL_TEMP}"
		echo "[$(gdate "+%H:%M:%S.%2N")] 🔄 Overriding with ${repo_name}/${file_name}..." >&2
	elif [[ "${casual_model_override}" == true ]]; then
		repo_name="${CASUAL_MODEL_REPO_NAME}"
		file_name="${CASUAL_MODEL_FILE_NAME}"
		temp="${CASUAL_MODEL_TEMP}"
		echo "[$(gdate "+%H:%M:%S.%2N")] 🔄 Overriding with ${repo_name}/${file_name}..." >&2
	elif [[ "${balanced_model_override}" == true ]]; then
		repo_name="${BALANCED_MODEL_REPO_NAME}"
		file_name="${BALANCED_MODEL_FILE_NAME}"
		temp="${BALANCED_MODEL_TEMP}"
		echo "[$(gdate "+%H:%M:%S.%2N")] 🔄 Overriding with ${repo_name}/${file_name}..." >&2
	elif [[ "${serious_model_override}" == true ]]; then
		repo_name="${SERIOUS_MODEL_REPO_NAME}"
		file_name="${SERIOUS_MODEL_FILE_NAME}"
		temp="${SERIOUS_MODEL_TEMP}"
		echo "[$(gdate "+%H:%M:%S.%2N")] 🔄 Overriding with ${repo_name}/${file_name}..." >&2
	fi

	### Generate the prompt ##############################################################################################
	prompt=$(
		generate_prompt \
			"${interactive}" \
			"${file_info}" \
			"${read_file}" \
			"${query}" \
			"${search_info}" \
			"${search_terms}" \
			"${add_system_info}" \
			"${add_directory_info}" \
			"${add_clipboard_info}"
	)

	### Kick off the LLM #################################################################################################
	start_llama_session \
		"${repo_name}" \
		"${file_name}" \
		"${prompt}" \
		"${interactive}" \
		"${verbose}" \
		"${verbose}" \
		"${new_tokens}" \
		"${temp}"

	### Print the elapsed time ###########################################################################################
	tput cuu1 && tput el;
	local end_time
	end_time=$(gdate +%s.%N)
	echo "[$(gdate "+%H:%M:%S.%2N")] ⌛️ Elapsed time: $(printf \
		"%.2f" "$(echo "${end_time} - ${start_time}" | bc)") seconds." >&2

	### Return success ###################################################################################################
	return 0
}
