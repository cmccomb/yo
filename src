#!/usr/bin/env zsh

########################################################################################################################
########################################################################################################################
######'                     `########'                ╭─────────────────────────────────────────╮                 `#####
####'    db    db  .d88b.     `####'                  │  Your AI Assistant in the Command Line  │                  `####
####     `8b  d8' .8P  Y8.     ####                   ╰─────────────────────────────────────────╯                   ####
####      `8bd8'  88    88     ####     If you are here, you are probably trying to fix or change something. If     ####
####        88    88    88     ####     you are here to fix something,  I am sorry it broke in the first place!     ####
####        88    `8b  d8'     ####     But either way, I hope that the documentation is helpful and I wish you     ####
####.       YP     `Y88P'     .####.     the best of luck. If you need help, please email: ccmcc2012@gmail.com     .####
######.                     .########.                                                                           .######
########################################################################################################################
########################################################################################################################

function yo() {

	######################################################################################################################
	### THE YO ###########################################################################################################
	######################################################################################################################

	local YO="✌️"
	readonly YO

	######################################################################################################################
	### THE VERSION ######################################################################################################
	######################################################################################################################

	### Define the version of the script #################################################################################

	### All versioning follows semver as defined at https://semver.org/ ##################################################
	local VERSION="0.1.0"
	readonly VERSION

	### Display version information ######################################################################################
	function show_version() {
		echo "yo v${VERSION} ${YO}"
	}

	######################################################################################################################
	### CONSTANTS AND SETTINGS ##########################################################################################
	######################################################################################################################

	local GENERAL_USERNAME="bartowski"
	local GENERAL_FINETUNING_STYLE="Instruct"
	local GENERAL_MODEL_FILETYPE="gguf"

	### Define parameters for the task model #############################################################################

	# Model parameters
	local TASK_USERNAME=$GENERAL_USERNAME
	local TASK_MODEL_SERIES="Llama-3.2"
	local TASK_MODEL_FINETUNING_STYLE=$GENERAL_FINETUNING_STYLE
	local TASK_MODEL_SIZE="1B"
	local TASK_MODEL_QUANT="Q4_K_M"
	local TASK_MODEL_FILETYPE=$GENERAL_MODEL_FILETYPE

	# Generation parameters
	local TASK_MODEL_NEW_TOKENS=16
	local TASK_MODEL_TEMP=0.2

	### Define parameters for the casual model ###########################################################################

	# Model parameters
	local CASUAL_GENERAL_USERNAME=$GENERAL_USERNAME
	local CASUAL_MODEL_SERIES="Qwen2.5-Coder"
	local CASUAL_MODEL_FINETUNING_STYLE=$GENERAL_FINETUNING_STYLE
	local CASUAL_MODEL_SIZE="3B"
	local CASUAL_MODEL_QUANT="Q4_K_M"
	local CASUAL_MODEL_FILETYPE=$GENERAL_MODEL_FILETYPE

	# Generation parameters
	local CASUAL_MODEL_NEW_TOKENS=128
	local CASUAL_MODEL_TEMP=0.2

	### Define parameters for the balanced model ###########################################################################
	local BALANCED_GENERAL_USERNAME=$GENERAL_USERNAME
	local BALANCED_MODEL_SERIES="Qwen2.5-Coder"
	local BALANCED_MODEL_FINETUNING_STYLE=$GENERAL_FINETUNING_STYLE
	local BALANCED_MODEL_SIZE="7B"
	local BALANCED_MODEL_QUANT="Q4_K_M"
	local BALANCED_MODEL_FILETYPE=$GENERAL_MODEL_FILETYPE

	# Generation parameters
	local BALANCED_MODEL_TEMP=0.2

	### Define the series model parameters ###############################################################################
	local SERIOUS_GENERAL_USERNAME=$GENERAL_USERNAME
	local SERIOUS_MODEL_SERIES="Qwen2.5-Coder"
	local SERIOUS_GENERAL_FINETUNING_STYLE=$GENERAL_FINETUNING_STYLE
	local SERIOUS_MODEL_SIZE="14B"
	local SERIOUS_MODEL_QUANT="IQ4_XS"
	local SERIOUS_GENERAL_MODEL_FILETYPE=$GENERAL_MODEL_FILETYPE

	# Generation parameters
	local SERIOUS_MODEL_NEW_TOKENS=512
	local SERIOUS_MODEL_TEMP=0.2

	### Custom search API key ############################################################################################
	local GOOGLE_CSE_API_KEY="AIzaSyBBXNq-DX1ENgFAiGCzTawQtWmRMSbDljk"
	local GOOGLE_CSE_ID="003333935467370160898:f2ntsnftsjy"
	local GOOGLE_CSE_BASE_URL="https://customsearch.googleapis.com/customsearch/v1"
	readonly GOOGLE_CSE_API_KEY GOOGLE_CSE_ID GOOGLE_CSE_BASE_URL

	### Maximum length of file content to extract ########################################################################
	local MAX_FILE_CONTENT_LENGTH=10000


  ### Set the repository and file names ################################################################################
  function compose_repo_and_model_name() {
    local username=$1 series=$2 size=$3 finetuning=$4 filetype=$5 quant=$6
    echo "${username}/${series}-${size}-${finetuning}-$(echo "${filetype}" | tr '[:lower:]' '[:upper:]') ${repo_name} ${series}-${size}-${finetuning}-${quant}.${filetype}"
	}


	# Set the repository and model names
	local TASK_MODEL_REPO_NAME TASK_MODEL_NAME
  read -r TASK_MODEL_REPO_NAME TASK_MODEL_NAME <<< "$(compose_repo_and_model_name \
    ${TASK_USERNAME} \
    ${TASK_MODEL_SERIES} \
    ${TASK_MODEL_SIZE} \
    ${TASK_MODEL_FINETUNING_STYLE} \
    ${TASK_MODEL_FILETYPE} \
    ${TASK_MODEL_QUANT}
  )"
  readonly TASK_MODEL_REPO_NAME TASK_MODEL_NAME

	# Set the repository and model names
	local CASUAL_MODEL_NAME CASUAL_MODEL_REPO_NAME
  read -r CASUAL_MODEL_REPO_NAME CASUAL_MODEL_NAME <<< "$(compose_repo_and_model_name \
    ${CASUAL_GENERAL_USERNAME} \
    ${CASUAL_MODEL_SERIES} \
    ${CASUAL_MODEL_SIZE} \
    ${CASUAL_MODEL_FINETUNING_STYLE} \
    ${CASUAL_MODEL_FILETYPE} \
    ${CASUAL_MODEL_QUANT}
  )"
	readonly CASUAL_MODEL_REPO_NAME CASUAL_MODEL_NAME

	# Set the repository and model names for the balanced model
	local BALANCED_MODEL_NAME BALANCED_MODEL_REPO_NAME
	read -r BALANCED_MODEL_REPO_NAME BALANCED_MODEL_NAME <<< "$(compose_repo_and_model_name \
    ${BALANCED_GENERAL_USERNAME} \
    ${BALANCED_MODEL_SERIES} \
    ${BALANCED_MODEL_SIZE} \
    ${BALANCED_MODEL_FINETUNING_STYLE} \
    ${BALANCED_MODEL_FILETYPE} \
    ${BALANCED_MODEL_QUANT}
  )"
	readonly BALANCED_MODEL_REPO_NAME BALANCED_MODEL_NAME


	# Set the repository and model names for the serious model
	local SERIOUS_MODEL_NAME SERIOUS_MODEL_REPO_NAME
	read -r SERIOUS_MODEL_REPO_NAME SERIOUS_MODEL_NAME <<< "$(compose_repo_and_model_name \
    ${SERIOUS_GENERAL_USERNAME} \
    ${SERIOUS_MODEL_SERIES} \
    ${SERIOUS_MODEL_SIZE} \
    ${SERIOUS_GENERAL_FINETUNING_STYLE} \
    ${SERIOUS_GENERAL_MODEL_FILETYPE} \
    ${SERIOUS_MODEL_QUANT}
  )"
	readonly SERIOUS_MODEL_REPO_NAME SERIOUS_MODEL_NAME

	######################################################################################################################
	### UTILITY FUNCTIONS ################################################################################################
	######################################################################################################################

	### Display usage instructions #######################################################################################
	function show_help() {
		cat <<-EOF
			yo - A command-line AI assistant.

			Usage:
			  yo [options] [question]

			Description:
			  If a question is provided, Yo will answer the question. Otherwise, Yo will enter an interactive session.

			Context Options:
			  These options help you control the information that Yo uses to answer your question.
			    -c, --clipboard         Copy the contents of the clipboard into Yo's context.
			    -d, --directory         Include a list of the files in the current directory in Yo's context.
			    -r, --read "PATH"       Extract the specified file or URL and into Yo's context. Supports text-based files
			                            (e.g., .txt, .md, .py), PDF files via pdftotext, and web pages via curl and pandoc.
			    -s, --search "TERMS"    Perform a web search using the specified quoted terms and integrate the results into
			                            Yo's context. Requires an active internet connection.
			    -S, --surf              Perform a web search using LLM-chosen terms based on the question. Integrates results
			                            into Yo's context. Requires an active internet connection.
			    -y, --system            Run a few system commands and integrate the information into Yo's context.

			Model Size Options:
			  Yo uses the following models:
			    * Task model (${TASK_MODEL_NAME})
			    * Casual model (${CASUAL_MODEL_NAME})
			    * Balanced model (${BALANCED_MODEL_NAME})
			    * Serious model (${SERIOUS_MODEL_NAME})
			  By default, Yo uses the task model for summarization and other small tasks, the casual model for one-off
			  questions, and the serious model for interactive sessions. You can override the default model by using the
			  following options:
			    -tm, --task-model       Use the task model
			    -cm, --casual-model     Use the casual model
			    -bm, --balanced-model   Use the balanced model
			    -sm, --serious-model    Use the serious model

			General Purpose Options:
			  These options provide general functionality.
			    -h, --help              Show this help message and exit.
			    -v, --verbose           Enable verbose mode for detailed output.
			    -V, --version           Show the version and exit.

			Examples:
			  * Answer a question:
			    $ yo "What is the capital of France?"

			  * For simple cases, you can omit the quotes:
			    $ yo what is the capital of france

			  * Start an interactive session:
			    $ yo

			  * Integrate information from a file:
			    $ yo --read src "How can I improve this source code?"

			  * Integrate information from a URL:
			    $ yo --read https://en.wikipedia.org/wiki/Paris "how big is paris"

			  * Integrate Google search results:
			    $ yo --search "what is the capital of Tobago"

			  * Use LLM-selected search terms:
			    $ yo --surf tell me about renewable energy trends.

			  * Combine context sources:
			    $ yo --read src --search "what is the capital of france"

			  * Add verbosity to any of these commands
			    $ yo what is the capital of france --verbose
		EOF
		return 0
	}

	# Check if the input is empty
	function check_nonempty() {

		# Parse arguments
		local variable_name=$1
		local variable_value=$2

		# Check if the input is empty
		if [[ -z "${variable_value}" ]]; then
			echo "Error: Invalid input ${variable_name}=\"${variable_value}\", expected a non-empty string." >&2
			return 1
		else
			return 0
		fi
	}

	# Check if the input is an integer and print an error message with the name of the variable if not
	function check_integer() {

		# Parse arguments
		local variable_name=$1
		local variable_value=$2

		# Check that inputs are non-empty
		check_nonempty "variable_name" "${variable_name}" || return 1
		check_nonempty "variable_value" "${variable_value}" || return 1

		# Check if the input is an integer
		if [[ ! "${variable_value}" =~ ^[0-9]+$ ]]; then
			echo "Error in ${funcstack[2]}: Invalid input ${variable_name}=\"${variable_value}\", expected an integer." >&2
			return 1
		else
			return 0
		fi
	}

	# Check if the input is Boolean
	function check_boolean() {

		# Parse arguments
		local variable_name=$1
		local variable_value=$2

		# Check that inputs are non-empty
		check_nonempty "variable_name" "${variable_name}" || return 1
		check_nonempty "variable_value" "${variable_value}" || return 1

		# Check if the input is Boolean
		if [[ "${variable_value}" != true && "${variable_value}" != false ]]; then
			echo "Error in ${funcstack[2]}: Invalid input ${variable_name}=\"${variable_value}\", expected a boolean." >&2
			return 1
		else
			return 0
		fi
	}


	# Check if the input is a float
	function check_float() {

		# Parse arguments
		local variable_name=$1
		local variable_value=$2

		# Check that inputs are non-empty
		check_nonempty "variable_name" "${variable_name}" || return 1
		check_nonempty "variable_value" "${variable_value}" || return 1

		# Check if the input is a float
		if [[ ! $variable_value =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
			echo "Error in ${funcstack[2]}: Invalid input ${variable_name}=\"${variable_value}\", expected a float." >&2
			return 1
		else
			return 0
		fi
	}

	# Function that checks to see if a model exists and download it if not
	function check_model_download() {
		# Parse arguments
		local repo_name=$1 model_name=$2 verbose=$3

		# Check that inputs are valid
		check_nonempty "repo_name" "${repo_name}" || return 1
		check_nonempty "model_name" "${model_name}" || return 1

    # Check if the model exists
    local model_path="/Users/${USER}/Library/Caches/llama.cpp/${repo_name//\//_}_${model_name}"
    if [[ ! -f "${model_path}" ]]; then
      if [[ "${verbose}" == true ]]; then
        if ! llama-cli --hf-repo "${repo_name}" --hf-file "${model_name}" -p "hi" -n 0; then
          echo "Error in ${funcstack[3]}: Failed to download ${repo_name}/${model_name}." >&2
          return 1
        fi
      else
        if ! llama-cli --hf-repo "${repo_name}" --hf-file "${model_name}" --no-warmup -p "hi" -n 0 2>/dev/null; then
          echo "Error in ${funcstack[3]}: Failed to download ${repo_name}/${model_name}." >&2
          return 1
        fi
      fi
    fi
	}

	######################################################################################################################
	### SEARCH ###########################################################################################################
	######################################################################################################################

	### Perform a web search with user-provided terms ####################################################################
	function perform_search() {

		# Parse arguments
		local terms=$1

		# Check that inputs are valid
		check_nonempty "terms" "${terms}" || return 1

		# Make variables
		local url response

		# Example API call
		url="${GOOGLE_CSE_BASE_URL}?key=${GOOGLE_CSE_API_KEY}&cx=${GOOGLE_CSE_ID}&q=${terms// /%20}"

		# Perform search and extract relevant information
		if ! response=$( \curl -s "${url}" | grep -E "^      \"title\"|^      \"snippet\""); then
			echo "Error: Failed to perform web search." >&2
			return 1
		else
      # Return response
      echo "${response}"
      return 0
    fi
	}

	### Extract optimized search terms using a small model ###############################################################
	function extract_search_terms() {

		# Parse arguments
		local query=$1 verbose=$2

		# Check that inputs are valid
		check_nonempty "query" "${query}" || return 1
		check_boolean "verbose" "${verbose}" || return 1

		# Make variables
		local prompt="" terms

		# Generate prompt
		prompt+="Your task is to extract the most relevant search terms from the following query for a web search.\n\n"
		prompt+="Here is an example:\n"
		prompt+="User Query: how large is the capital of france ${YO}\n"
		prompt+="Search Terms: paris capital population area\n\n"
		prompt+="Here is another example:\n"
		prompt+="User Query: what is the furthest planet from the sun ${YO}\n"
		prompt+="Search Terms: solar system furthest planet distance\n\n"
		prompt+="Here is the real user query.\n"
		prompt+="User Query: ${query}\n"
		prompt+="Search Terms: "

		# Generate response
		terms=$(
			start_llama_session \
				"${TASK_MODEL_REPO_NAME}" \
				"${TASK_MODEL_NAME}" \
				"${prompt}" \
				false \
				"${verbose}" \
				false \
				"${TASK_MODEL_NEW_TOKENS}" \
				"${TASK_MODEL_TEMP}"
		)

		# Only take the first line
		terms=$(echo "${terms}" | head -n 1)

		# Remove [end of text] marker if needed
		terms="${terms//\[end of text\]/}"

		# Return results
		echo "${terms}"
		return 0
	}

	######################################################################################################################
	### CONTENT EXTRACTION ###############################################################################################
	######################################################################################################################

	### Extract file_info from a file or URL (supports text and PDF files) ###############################################
	function extract_file_info() {

		# Parse arguments
		local source=$1 max_length=$2

		# Check that inputs are valid
		check_nonempty "source" "${source}" || return 1
		check_integer "max_length" "${max_length}" || return 1

		# Make variables
		local file_info=""

		if [[ "${source}" =~ ^https?:// ]]; then
			# Fetch file_info from URL
			if ! file_info=$(curl -s "${source}" | pandoc -f html -t plain --quiet); then
				echo "Error: Failed to fetch file_info from URL." >&2
				return 1
			fi
		else
			# Fetch file_info from file
			[[ ! -f "${source}" ]] && {
				echo "Error: File not found." >&2
				return 1
			}

			case $source in
			*.pdf)
				command -v pdftotext >/dev/null 2>&1 || {
					echo "Error: pdftotext not installed. Install it using your package manager (e.g., brew install poppler)." >&2
					return 1
				}
				file_info=$(pdftotext "${source}" - 2>/dev/null)
				;;
			*.txt | *)
				file_info=$(cat "${source}")
				;;
			esac
		fi

		# Trim to max length if needed
		[[ -n "${max_length}" && "${max_length}" -gt 0 ]] && file_info=${file_info:0:$max_length}

		# Return file_info
		echo "${file_info}"
		return 0
	}

	######################################################################################################################
	### LLMs AND PROMPTS #################################################################################################
	######################################################################################################################

	function generate_system_info_context() {
	  local model cores ram free_storage
	  model=$(system_profiler SPHardwareDataType | grep "Model Name" | awk -F": " '{print $2}')
	  cores=$(sysctl -n hw.ncpu)
	  ram=$(sysctl -n hw.memsize | awk '{x=$1/1024/1024/1024; print x}')
	  free_storage=$(df -h / | tail -1 | awk '{split($4, a, "G"); print a[1]}')
	  echo "You are on a ${model} with ${cores} cores, ${ram}GB RAM, and ${free_storage}GB free disk space."
	}

	function generate_directory_info_context() {
	  	echo "================= BEGINNING OF CURRENT DIRECTORY CONTENTS =================\n"
			prompt+="You were invoked from the $(pwd) directory. "
			prompt+="Here are the contents (truncated at 50, sorted by file size:\n"
			prompt+="$(ls -lahpSR | head -n 50)\n\n"
			prompt+="Here is a preview of the contents of the largest readable files:\n"
			prompt+="$( \
			  find . -maxdepth 1 -type f -exec file --mime {} + \
          | grep 'text/' \
          | cut -d: -f1 \
          | xargs du -h \
          | sort -rh \
          | head -n 5 \
          | awk '{print $2}' \
          | xargs -I {} sh -c 'echo "\n\nFile: {}"; echo ---\\n$(head -n 10 "{}")\\n---'
      )\n"
			prompt+="==================== END OF CURRENT DIRECTORY CONTENTS ====================\n\n"

	}

	### Generate a prompt for one-off or interactive sessions ############################################################
	function generate_prompt() {

		# Parse arguments
		local interactive=$1 file_info=$2 filename=$3 query=$4 search_info=$5 search_terms=$6 add_system_info=$7 \
			add_directory_info=$8 add_clipboard_info=$9

		# Check that inputs are valid
		check_boolean "interactive" "${interactive}" || return 1
		check_boolean "add_system_info" "${add_system_info}" || return 1
		check_boolean "add_directory_info" "${add_directory_info}" || return 1
		check_boolean "add_clipboard_info" "${add_clipboard_info}" || return 1

		# Make variables
		local prompt=""

		# Generate basic prompt
		prompt+="You are playing the role of Yo, a highly-capable AI assistant living in the MacOS terminal. "
		prompt+="It is currently $(date).\n\n"

		# Add system information if requested
		if [[ "$add_system_info" == true ]]; then
			prompt+=$(generate_system_info_context)
		fi

		# Add directory information if requested
		if [[ "${add_directory_info}" == true ]]; then
			prompt+="================= BEGINNING OF CURRENT DIRECTORY CONTENTS =================\n"
			prompt+="You were invoked from the $(pwd) directory. "
			prompt+="Here are the contents (truncated at 50, sorted by file size:\n"
			prompt+="$(ls -lahpSR | head -n 50)\n\n"
			prompt+="Here is a preview of the contents of the largest readable files:\n"
			prompt+="$( \
			  find . -maxdepth 1 -type f -exec file --mime {} + \
          | grep 'text/' \
          | cut -d: -f1 \
          | xargs du -h \
          | sort -rh \
          | head -n 5 \
          | awk '{print $2}' \
          | xargs -I {} sh -c 'echo "\n\nFile: {}"; echo ---\\n$(head -n 10 "{}")\\n---'
      )\n"
			prompt+="==================== END OF CURRENT DIRECTORY CONTENTS ====================\n\n"
		fi

		# Add clipboard information if requested
		if [[ "${add_clipboard_info}" == true ]]; then
			prompt+="================= BEGINNING OF CLIPBOARD CONTENTS =================\n"
			prompt+="Here is the contents of the clipboard:\n"
			prompt+="$(pbpaste)\n"
			prompt+="==================== END OF CLIPBOARD CONTENTS ====================\n\n"
		fi

		# Add file file_info if available
		if [[ -n "${file_info}" ]]; then
			prompt+="Relevant information from ${filename}:\n"
			prompt+="================= BEGINNING OF FILE CONTENTS =================\n"
			prompt+="${file_info}\n"
			prompt+="==================== END OF FILE CONTENTS ====================\n\n"
		fi

		# Add search information if available
		if [[ -n "${search_terms}" ]]; then
			prompt+="Relevant information from web search using ${search_terms}:\n"
			prompt+="================= BEGINNING OF SEARCH RESULTS =================\n"
			prompt+="${search_info}\n"
			prompt+="==================== END OF SEARCH RESULTS ====================\n\n"
		fi

		# Add query and instructions based on interactive ##################################################################
		if [[ "${interactive}" == false ]]; then
			prompt+="Your task is to directly answer the user's question. "
			prompt+="Your answer will be concise, helpful, and immediately usable. "
			prompt+="End your answer with the symbol ${YO}.\n\n"
			prompt+="Here is an example:\n"
			prompt+="User Query: how large is the capital of france ${YO}\n"
			prompt+="Your Super-Short Answer: 41 square miles (105 square km) ${YO}\n\n"
			prompt+="Here is another example:\n"
			prompt+="User Query: what is the furthest planet from the sun ${YO}\n"
			prompt+="Search Terms:  Neptune is the furthest planet from the Sun. ${YO}\n\n"
			prompt+="Here is the real user query.\n"
			prompt+="User Query: ${query} ${YO}\n"
			prompt+="Your Super-Short Answer:"
		else
			prompt+="Your task is to assist the user in an interactive session, responding concisely and accurately."
		fi

		# Return prompt
		echo "${prompt}"
		return 0
	}

	### Start a llama-cli session ########################################################################################
	function start_llama_session() {

		# Parse arguments
		local repo_name=$1 model_name=$2 prompt=$3 interactive=$4 squash_stderr=$5 display_prompt=$6 \
			number_of_tokens_to_generate=$7 temp=$8

		# Check that inputs are valid
		check_nonempty "repo_name" "${repo_name}" || return 1
		check_nonempty "model_name" "${model_name}" || return 1
		check_nonempty "prompt" "${prompt}" || return 1
		check_boolean "interactive" "${interactive}" || return 1
		check_boolean "squash_stderr" "${squash_stderr}" || return 1
		check_boolean "display_prompt" "${display_prompt}" || return 1
		check_integer "number_of_tokens_to_generate" "${number_of_tokens_to_generate}" || return 1
		check_float "temp" "${temp}" || return 1

		# Check if the model exists and download it if not
		check_model_download "${repo_name}" "${model_name}" "${squash_stderr}" || return 1

		# Configure llama-cli arguments
		local args=(
			--threads "$(sysctl -n hw.logicalcpu_max)"
			--hf-repo "$repo_name"
			--hf-file "$model_name"
			--prompt "$prompt"
			--prompt-cache "/tmp/yo_prompt_cache_${model_name}"
			--predict "${number_of_tokens_to_generate}"
			--temp "${temp}"
			--seed 42
			--prio 3
			--mirostat 2
			--flash-attn
			--no-warmup
		)

		# Add conversation or reverse-prompt based on mode
		if [[ "${interactive}" == true ]]; then
			args+=(--conversation)
		else
			args+=(--reverse-prompt "${YO}")
		fi

		# Display prompt
		if [[ "${display_prompt}" != true ]]; then
			args+=(--no-display-prompt)
		else
			args+=(--verbose-prompt)
		fi

		# Start session
		if [[ "${squash_stderr}" != true ]]; then
			if ! llama-cli "${args[@]}" 2>/dev/null; then
				echo "Error: llama-cli command failed while attempting to call ${repo_name}/${model_name}." >&2
				return 1
			fi
		else
			if ! llama-cli "${args[@]}"; then
				echo "Error: llama-cli command failed while attempting to call ${repo_name}/${model_name}." >&2
				return 1
			fi
		fi
	}

	######################################################################################################################
	### MAIN FUNCTION ####################################################################################################
	######################################################################################################################

	### Parse arguments ##################################################################################################

	# Define variables
	local verbose=false query=""
	local read_mode=false read_file="" file_info=""
	local surf_mode=false search_mode=false search_terms="" search_info=""
	local add_directory_info=false add_system_info=false add_clipboard_info=false
	local task_model_override=false casual_model_override=false balanced_model_override=false
	local serious_model_override=false
	local quant size repo_name model_name new_tokens temp prompt

	# Start parsing arguments
	while [[ $# -gt 0 ]]; do
		case $1 in

		# Early exit with help message
		-h | --help)
			show_help
			return 0
			;;

		# Early exit with version information
		-V | --version)
			show_version
			return 0
			;;

		# Read in a file
		-r | --read)
			read_mode=true
			if [[ -n $2 && ! $2 =~ ^- ]]; then
				read_file=$2
				shift
			else
				echo "Error: --read requires a file." >&2
				return 1
			fi
			;;

		# Do some searching
		-s | --search)
			search_mode=true
			if [[ -n $2 && $2 =~ ^".*"$ ]]; then
				search_terms=$2
				shift
			else
				echo "Error: --search requires quoted terms." >&2
				return 1
			fi
			;;

		# Make the output verbose
		-v | --verbose) verbose=true ;;

		# Surf the web with LLM-defined search terms
		-S | --surf) surf_mode=true ;;

		# Add system information to the context
		-y | --system) add_system_info=true ;;

		# Add directory information to the context
		-d | --directory) add_directory_info=true ;;

		# Add clipboard information to the context
		-c | --clipboard) add_clipboard_info=true ;;

		# Use the task model
		-tm | --task-model) task_model_override=true ;;

		# Use the casual model
		-cm | --casual-model) casual_model_override=true ;;

		# Use the balanced model
		-bm | --balanced-model) balanced_model_override=true ;;

		# Use the serious model
		-sm | --serious-model) serious_model_override=true ;;

		# If its something that looks like a flag but isn't one of the above, show an error
		-*)
			echo "Error: Unknown flag: $1" >&2
			return 1
			;;

		# If its not a flag, add it to the general query
		*) query+="$1 " ;;
		esac
		shift
	done

	### Read in files if needed ##########################################################################################
	if [[ "${read_mode}" == true && -n "${read_file}" ]]; then
		file_info=$(extract_file_info "${read_file}" "${MAX_FILE_CONTENT_LENGTH}") || return 1
	fi

	### Bring in search information if needed ############################################################################
	if [[ "${search_mode}" == true ]]; then
		search_info=$(perform_search "${search_terms}")
	elif [[ "${surf_mode}" == true ]]; then
		search_terms=$(extract_search_terms "${query}" "${verbose}")
		search_info=$(perform_search "${search_terms}")
	fi

	### Configure the model based on whether its a one-off or interactive session ########################################
	if [[ -n "${query}" ]]; then
		repo_name="${CASUAL_MODEL_REPO_NAME}"
		model_name="${CASUAL_MODEL_NAME}"
		new_tokens="${CASUAL_MODEL_NEW_TOKENS}"
		temp="${CASUAL_MODEL_TEMP}"
		interactive=false
	else
		repo_name="${SERIOUS_MODEL_REPO_NAME}"
		model_name="${SERIOUS_MODEL_NAME}"
		new_tokens="${SERIOUS_MODEL_NEW_TOKENS}"
		temp=${SERIOUS_MODEL_TEMP}
		interactive=true
	fi

	### Override the model if needed ######################################################################################
	if [[ "${task_model_override}" == true ]]; then
		repo_name="${TASK_MODEL_REPO_NAME}"
		model_name="${TASK_MODEL_NAME}"
		temp="${TASK_MODEL_TEMP}"
	elif [[ "${casual_model_override}" == true ]]; then
		repo_name="${CASUAL_MODEL_REPO_NAME}"
		model_name="${CASUAL_MODEL_NAME}"
		temp="${CASUAL_MODEL_TEMP}"
	elif [[ "${balanced_model_override}" == true ]]; then
		repo_name="${BALANCED_MODEL_REPO_NAME}"
		model_name="${BALANCED_MODEL_NAME}"
		temp="${BALANCED_MODEL_TEMP}"
	elif [[ "${serious_model_override}" == true ]]; then
		repo_name="${SERIOUS_MODEL_REPO_NAME}"
		model_name="${SERIOUS_MODEL_NAME}"
		temp="${SERIOUS_MODEL_TEMP}"
	fi

	### Generate the prompt ##############################################################################################
	prompt=$(
		generate_prompt \
			"${interactive}" \
			"${file_info}" \
			"${read_file}" \
			"${query}" \
			"${search_info}" \
			"${search_terms}" \
			"${add_system_info}" \
			"${add_directory_info}" \
			"${add_clipboard_info}"
	)

	### Kick off the LLM #################################################################################################
	start_llama_session \
		"${repo_name}" \
		"${model_name}" \
		"${prompt}" \
		"${interactive}" \
		"${verbose}" \
		"${verbose}" \
		"${new_tokens}" \
		"${temp}"

	### Return success ###################################################################################################
	return 0
}
