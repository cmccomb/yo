#!/usr/bin/env zsh

########################################################################################################################
########################################################################################################################
######'                     `########'                                                                            `#####
####'    db    db  .d88b.     `####'                      Your AI Assistant in the Command Line                    `####
####     `8b  d8' .8P  Y8.     ####                                                                                 ####
####      `8bd8'  88    88     ####     If you are here, you are probably trying to fix or change something. If     ####
####        88    88    88     ####     you are here to fix something,  I am sorry it broke in the first place!     ####
####        88    `8b  d8'     ####     But either way, I hope that the documentation is helpful and I wish you     ####
####.       YP     `Y88P'     .####.    the best of luck. If you need help, please email: <ccmcc2012@gmail.com>    .####
######.                     .########.                                                                           .######
########################################################################################################################
########################################################################################################################

function yo() {

	######################################################################################################################
	### UTILITIES AND CONSTANTS ##########################################################################################
	######################################################################################################################

	### Display version information ######################################################################################
	local VERSION="0.1.0"
	function show_version() {
		echo "yo v$VERSION $YO"
	}

	### Symbol used to signal the end of one-off sessions ################################################################
	local YO="✌️"

	### Common model parameters ##########################################################################################
	local MODEL_USERNAME="bartowski"
	local MODEL_SERIES="Qwen2.5"
	local MODEL_FINETUNING="Instruct"
	local MODEL_FILETYPE="gguf"

	### Define parameters for the task model #############################################################################
	local TASK_MODEL_USERNAME=$MODEL_USERNAME
	local TASK_MODEL_SERIES="Llama-3.2"
	local TASK_MODEL_FINETUNING=$MODEL_FINETUNING
	local TASK_MODEL_SIZE="1B"
	local TASK_MODEL_QUANT="Q4_K_M"
	local TASK_MODEL_FILETYPE=$MODEL_FILETYPE
	local TASK_MODEL_NEW_TOKENS=16
	local TASK_MODEL_TEMP=0.2

	### Define parameters for the casual model ###########################################################################
	local CASUAL_MODEL_USERNAME=$MODEL_USERNAME
	local CASUAL_MODEL_SERIES=$MODEL_SERIES
	local CASUAL_MODEL_FINETUNING=$MODEL_FINETUNING
	local CASUAL_MODEL_SIZE="3B"
	local CASUAL_MODEL_QUANT="Q4_K_M"
	local CASUAL_MODEL_FILETYPE=$MODEL_FILETYPE
	local CASUAL_MODEL_NEW_TOKENS=128
	local CASUAL_MODEL_TEMP=0.2

	### Define the series model parameters ###############################################################################
	local SERIOUS_MODEL_USERNAME=$MODEL_USERNAME
	local SERIOUS_MODEL_SERIES=$MODEL_SERIES
	local SERIOUS_MODEL_FINETUNING=$MODEL_FINETUNING
	local SERIOUS_MODEL_SIZE="14B"
	local SERIOUS_MODEL_QUANT="IQ4_XS"
	local SERIOUS_MODEL_FILETYPE=$MODEL_FILETYPE
	local SERIOUS_MODEL_NEW_TOKENS=512
	local SERIOUS_MODEL_TEMP=0.2

	### Custom search API key ############################################################################################
	local GOOGLE_CSE_API_KEY="AIzaSyBBXNq-DX1ENgFAiGCzTawQtWmRMSbDljk"
	local GOOGLE_CSE_ID="003333935467370160898:f2ntsnftsjy"
	local GOOGLE_CSE_BASE_URL="https://customsearch.googleapis.com/customsearch/v1"

	### Maximum length of file content to extract ########################################################################
	local MAX_FILE_CONTENT_LENGTH=10000

	### Display usage instructions #######################################################################################
	function show_help() {
		cat <<-EOF
			yo - A command-line AI assistant.

			Usage:
			  yo [options] [question]

			Description:
			  If a question is provided, Yo will answer the question. Otherwise, Yo will enter an interactive session.

			Context Options:
			  These options help you control the information that Yo uses to answer your question.
			  -c, --clipboard         Copy the contents of the clipboard into Yo's context.
			  -d, --directory         List the files of the current directory in Yo's context.
			  -r, --read "PATH"       Extract the specified file or URL and into Yo's context. Supports text-based files
			                          (e.g., .txt, .md, .py), PDF files via pdftotext, and web pages via curl and pandoc.
			  -s, --search "TERMS"    Perform a web search using the specified quoted terms and integrate the results into
			                          Yo's context. Requires an active internet connection.
			  -S, --surf              Perform a web search using LLM-chosen terms based on the question. Integrates results
			                          into Yo's context. Requires an active internet connection.
			  -y, --system            Perform several system commands and integrate the information into Yo's context.

			General Purpose Options:
			  These options provide general functionality.
			  -h, --help              Show this help message and exit.
			  -v, --verbose           Enable verbose mode for detailed output.
			  -V, --version           Show the version and exit.

			Examples:
			  1. Answer a question:
			    $ yo "What is the capital of France?"

			  2. For simple cases, you can omit the quotes:
			    $ yo what is the capital of france

			  3. Start an interactive session:
			    $ yo

			  4. Integrate information from a file:
			    $ yo --read src "How can I improve this source code?"

			  5. Integrate information from a URL:
			    $ yo --read https://en.wikipedia.org/wiki/Paris "how big is paris"

			  6. Integrate Google search results:
			    $ yo --search "what is the capital of Tobago"

			  7. Use LLM-selected search terms:
			    $ yo --surf tell me about renewable energy trends.

			  8. Combine context sources:
			    $ yo --read src --search "what is the capital of france"

			  9. Add verbosity to any of these commands
			    $ yo what is the capital of france --verbose
		EOF
	}

	### Compose the repository name based on the model parameters ########################################################
	function compose_repo_name() {

		# Parse arguments
		local username=$1 series=$2 size=$3 finetuning=$4 filetype=$5

		# Return the repository name
		echo "${username}/${series}-${size}-${finetuning}-$(echo $filetype | tr '[:lower:]' '[:upper:]')"
	}

	### Compose the model name based on the model parameters #############################################################
	function compose_model_name() {

		# Parse arguments
		local series=$1 size=$2 finetuning=$3 quant=$4 filetype=$5

		# Return the model name
		echo "${series}-${size}-${finetuning}-${quant}.${filetype}"
	}

	######################################################################################################################
	### SEARCH ###########################################################################################################
	######################################################################################################################

	### Perform a web search with user-provided terms ####################################################################
	function perform_search() {

		# Parse arguments
		local terms=$1

		# Make variables
		local url search_term_slug response

		# Generate search URL
		search_term_slug="${terms// /%20}"

		# Example API call
		url="$GOOGLE_CSE_BASE_URL?key=$GOOGLE_CSE_API_KEY&cx=$GOOGLE_CSE_ID&q=$search_term_slug"

		# Perform search and extract relevant information
		response=$(curl -s "$url" | grep -E "^      \"title\"|^      \"snippet\"")

		# Return response
		echo "$response"
	}

	### Extract optimized search terms using a small model ###############################################################
	function extract_search_terms() {

		# Parse arguments
		local query=$1 verbose=$2

		# Generate prompt
		local prompt=""
		prompt+="Your task is to extract the most relevant search terms from the following query for a web search.\n\n"
		prompt+="Here is an example:\n"
		prompt+="User Query: how large is the capital of france $YO\n"
		prompt+="Search Terms: paris capital population area\n\n"
		prompt+="Here is another:\n"
		prompt+="User Query: what is the furthest planet from the sun $YO\n"
		prompt+="Search Terms: solar system furthest planet distance\n\n"
		prompt+="Here is the real one.\n"
		prompt+="User Query: $query\n"
		prompt+="Search Terms: "

		# Generate the repo name for the task model
		repo_name="$(
			compose_repo_name \
				$TASK_MODEL_USERNAME \
				$TASK_MODEL_SERIES \
				$TASK_MODEL_SIZE \
				$TASK_MODEL_FINETUNING \
				$TASK_MODEL_FILETYPE
		)"

		# Generate the model name for the task model
		model_name="$(
			compose_model_name \
				$TASK_MODEL_SERIES \
				$TASK_MODEL_SIZE \
				$TASK_MODEL_FINETUNING \
				$TASK_MODEL_QUANT \
				$TASK_MODEL_FILETYPE
		)"

		# Generate response
		local terms
		terms=$(start_llama_session "$repo_name" "$model_name" "$prompt" false "$verbose" false $TASK_MODEL_NEW_TOKENS $TASK_MODEL_TEMP)

		# Only take the first line
		terms=$(echo "$terms" | head -n 1)

		# Remove [end of text] marker if needed
		terms="${terms//\[end of text\]/}"

		# Return results
		echo "$terms"
	}

	######################################################################################################################
	### CONTENT EXTRACTION ###############################################################################################
	######################################################################################################################

	### Extract file_info from a file or URL (supports text and PDF files) ###############################################
	function extract_file_info() {

		# Parse arguments
		local source=$1 max_length=$2

		# Make variables
		local file_info

		if [[ $source =~ ^https?:// ]]; then
			# Fetch file_info from URL
			if ! file_info=$(curl -s "$source" | pandoc -f html -t plain --quiet); then
				echo "Error: Failed to fetch file_info from URL." >&2
				return 1
			fi
		else
			# Fetch file_info from file
			[[ ! -f "$source" ]] && {
				echo "Error: File not found." >&2
				return 1
			}

			case $source in
			*.pdf)
				command -v pdftotext >/dev/null 2>&1 || {
					echo "Error: pdftotext not installed. Install it using your package manager (e.g., brew install poppler)." >&2
					return 1
				}
				file_info=$(pdftotext "$source" - 2>/dev/null)
				;;
			*.txt | *)
				file_info=$(cat "$source")
				;;
			esac
		fi

		# Trim to max length if needed
		[[ -n "$max_length" && "$max_length" -gt 0 ]] && file_info=${file_info:0:$max_length}

		# Return file_info
		echo "$file_info"
	}

	######################################################################################################################
	### LLMs AND PROMPTS #################################################################################################
	######################################################################################################################

	### Generate a prompt for one-off or interactive sessions ############################################################
	function generate_prompt() {

		# Parse arguments
		local interactive=$1 file_info=$2 filename=$3 query=$4 search_info=$5 search_terms=$6 add_system_info=$7 \
			add_directory_info=$8 add_clipboard_info=$9

		# Generate basic prompt
		local prompt=""
		prompt+="You are playing the role of Yo, a highly-capable AI assistant living in the MacOS terminal. "
		prompt+="It is currently $(date).\n\n"

		# Add system information if requested
		if [[ $add_system_info == true ]]; then
			prompt+="You were invoked on a $(system_profiler SPHardwareDataType | grep "Model Name" | awk -F": " '{print $2}') "
			prompt+="with $(sysctl -n hw.ncpu) cores, "
			prompt+="$(sysctl -n hw.memsize | awk '{x=$1/1024/1024/1024; print x}')GB RAM, "
			prompt+="and $(df -h / | tail -1 | awk '{split($4, a, "G"); print a[1]}')GB free disk space.\n\n"
		fi

		# Add directory information if requested
		if [[ $add_directory_info == true ]]; then
			prompt+="================= BEGINNING OF CURRENT DIRECTORY CONTENTS =================\n"
			prompt+="You were invoked from the $(pwd) directory. "
			prompt+="Here are the contents (truncated at 50, sorted by file size:\n"
			prompt+="$(ls -lahpS | head -n 50)\n\n"
			prompt+="==================== END OF CURRENT DIRECTORY CONTENTS ====================\n\n"
		fi

		# Add clipboard information if requested
		if [[ $add_clipboard_info == true ]]; then
			prompt+="================= BEGINNING OF CLIPBOARD CONTENTS =================\n"
			prompt+="Here is the contents of the clipboard:\n"
			prompt+="$(pbpaste)\n"
			prompt+="==================== END OF CLIPBOARD CONTENTS ====================\n\n"
		fi

		# Add file file_info if available
		if [[ -n "$file_info" ]]; then
			prompt+="Relevant information from $()$filename$():\n"
			prompt+="================= BEGINNING OF FILE CONTENTS =================\n"
			prompt+="$file_info\n"
			prompt+="==================== END OF FILE CONTENTS ====================\n\n"
		fi

		# Add search information if available
		if [[ -n "$search_terms" ]]; then
			prompt+="Relevant information from web search using $()$search_terms$():\n"
			prompt+="================= BEGINNING OF SEARCH RESULTS =================\n"
			prompt+="$search_info\n"
			prompt+="==================== END OF SEARCH RESULTS ====================\n\n"
		fi

		# Add query and instructions based on interactive ################################################################
		if [[ $interactive == false ]]; then
			prompt+="Your task is to directly answer the user's question. "
			prompt+="Your answer will be concise, helpful, and immediately usable. "
			prompt+="End your answer with the stop symbol $YO.\n\n"
			prompt+="User Query: $query $YO\n"
			prompt+="Your Super-Short Answer:"
		else
			prompt+="Your task is to assist the user in an interactive session, responding concisely and accurately."
		fi

		# Return prompt
		echo "$prompt"
	}

	### Start a llama-cli session ########################################################################################
	function start_llama_session() {

		# Parse arguments
		local repo_name=$1 model_name=$2 prompt=$3 interactive=$4 squash_stderr=$5 display_prompt=$6 \
			number_of_tokens_to_generate=$7 temp=$8

		# Configure llama-cli arguments
		local args=(
			--threads "$(sysctl -n hw.logicalcpu_max)"
			--hf-repo "$repo_name"
			--hf-file "$model_name"
			--prompt "$prompt"
			--prompt-cache "/tmp/yo_prompt_cache_$repo_name_$model_name"
			--predict "$number_of_tokens_to_generate"
			--temp "$temp"
			--seed 42
			--prio 3
			--mirostat 2
			--flash-attn
			--no-warmup
		)

		# Add conversation or reverse-prompt based on mode
		if [[ $interactive == true ]]; then
			args+=(--conversation)
		else
			args+=(--reverse-prompt "$YO")
		fi

		# Display prompt
		if [[ $display_prompt != true ]]; then
			args+=(--no-display-prompt)
		else
			args+=(--verbose-prompt)
		fi

		# Start session
		if [[ $squash_stderr != true ]]; then
			llama-cli "${args[@]}" 2>/dev/null
		else
			llama-cli "${args[@]}"
		fi
	}

	######################################################################################################################
	### MAIN FUNCTION ####################################################################################################
	######################################################################################################################

	### Parse arguments ##################################################################################################
	local query="" verbose=false
	local read_file="" read_mode=false
	local search_mode=false search_terms="" surf_mode=false
	local add_directory_info=false add_system_info=false add_clipboard_info=false
	while [[ $# -gt 0 ]]; do
		case $1 in
		-h | --help)
			show_help
			return 0
			;;
		-r | --read)
			read_mode=true
			if [[ -n $2 && ! $2 =~ ^- ]]; then
				read_file=$2
				shift
			else
				echo "Error: --read requires a file." >&2
				return 1
			fi
			;;
		-v | --verbose) verbose=true ;;
		-V | --version)
			show_version
			return 0
			;;
		-s | --search)
			search_mode=true
			if [[ -n $2 && $2 =~ ^".*"$ ]]; then
				search_terms=$2
				shift
			else
				echo "Error: --search requires quoted terms." >&2
				return 1
			fi
			;;
		-S | --surf) surf_mode=true ;;
		-y | --system) add_system_info=true ;;
		-d | --directory) add_directory_info=true ;;
		-c | --clipboard) add_clipboard_info=true ;;
		*) query+="$1 " ;;
		esac
		shift
	done

	### Read in files if needed ##########################################################################################
	local file_info=""
	if [[ $read_mode == true && -n "$read_file" ]]; then
		file_info=$(extract_file_info "$read_file" $MAX_FILE_CONTENT_LENGTH) || return 1
	fi

	### Bring in search information if needed ############################################################################
	local search_info=""
	if [[ $search_mode == true ]]; then
		search_info=$(perform_search "$search_terms")
	elif [[ $surf_mode == true ]]; then
		search_terms=$(extract_search_terms "$query" "$verbose")
		search_info=$(perform_search "$search_terms")
	fi

	### Configure the model based on whether its a one-off or interactive session ########################################
	local quant size repo_name model_name new_tokens temp
	if [[ -n "$query" ]]; then
		repo_name="$(
			compose_repo_name \
				$CASUAL_MODEL_USERNAME \
				$CASUAL_MODEL_SERIES \
				$CASUAL_MODEL_SIZE \
				$CASUAL_MODEL_FINETUNING \
				$CASUAL_MODEL_FILETYPE
		)"
		model_name="$(
			compose_model_name \
				$CASUAL_MODEL_SERIES \
				$CASUAL_MODEL_SIZE \
				$CASUAL_MODEL_FINETUNING \
				$CASUAL_MODEL_QUANT \
				$CASUAL_MODEL_FILETYPE
		)"
		new_tokens="$CASUAL_MODEL_NEW_TOKENS"
		temp=$CASUAL_MODEL_TEMP
		interactive=false
	else
		repo_name="$(
			compose_repo_name \
				$SERIOUS_MODEL_USERNAME \
				$SERIOUS_MODEL_SERIES \
				$SERIOUS_MODEL_SIZE \
				$SERIOUS_MODEL_FINETUNING \
				$SERIOUS_MODEL_FILETYPE
		)"
		model_name="$(
			compose_model_name \
				$SERIOUS_MODEL_SERIES \
				$SERIOUS_MODEL_SIZE \
				$SERIOUS_MODEL_FINETUNING \
				$SERIOUS_MODEL_QUANT \
				$SERIOUS_MODEL_FILETYPE
		)"
		new_tokens="$SERIOUS_MODEL_NEW_TOKENS"
		temp=$SERIOUS_MODEL_TEMP
		interactive=true
	fi

	### Generate the prompt ##############################################################################################
	local prompt
	prompt=$(
		generate_prompt \
			$interactive \
			"$file_info" \
			"$read_file" \
			"$query" \
			"$search_info" \
			"$search_terms" \
			$add_system_info \
			$add_directory_info \
			$add_clipboard_info
	)

	### Kick off the LLM ##############################################################################################
	start_llama_session \
		"$repo_name" \
		"$model_name" \
		"$prompt" \
		$interactive \
		$verbose \
		$verbose \
		$new_tokens \
		$temp

}
