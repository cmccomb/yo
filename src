#!/usr/bin/env zsh

# `yo` - A command-line AI assistant
function yo() {

	# Symbol used to signal the end of one-off sessions
	local STOP_SYMBOL="✌️"

	# Display usage instructions
	function show_help() {
		cat <<-EOF
			yo - A command-line AI assistant.

			Usage:
			  yo [options] [question]

			Description:
			  If a question is provided, Yo will answer the question. Otherwise, Yo will enter an interactive session.

			Options:
			  -h, --help          Show this help message and exit.
			  -r, --read [file]   Integrate information from the specified file (supports text files and PDFs).
			  -v, --verbose       Enable verbose mode for detailed output.
		EOF
	}

	# Generate a prompt for one-off or interactive sessions
	function generate_prompt() {
		local mode=$1 content=$2 filename=$3 query=$4
		local prompt=""
		prompt+="You are playing the role of Yo, a highly-capable AI assistant for the MacOS terminal. "
		prompt+="It is currently $(date). "
		prompt+="You were invoked from the $(pwd) directory "
		prompt+="on a $(system_profiler SPHardwareDataType | grep "Model Name" | awk -F": " '{print $2}') "
		prompt+=" with: $(sysctl -n hw.ncpu ) cores, "
		prompt+="$(sysctl -n hw.memsize | awk '{x=$1/1024/1024/1024; print x}')GB RAM, "
		prompt+="and $( df -h / | tail -1 | awk '{split($4, a, "G"); print a[1]}')GB free disk space.\n\n"

		if [[ -n "$content" ]]; then
			prompt+="Relevant Information from file \"$filename\":\n"
			prompt+="================= BEGINNING OF FILE CONTENTS =================\n"
			prompt+="$content\n"
			prompt+="==================== END OF FILE CONTENTS ====================\n\n"
		fi

		if [[ $mode == "one-off" ]]; then
			prompt+="Your task is to answer the user's question directly. "
			prompt+="Your answer will be concise, helpful, and actionable. "
			prompt+="End your answer with the stop symbol '$STOP_SYMBOL'.\n\n"
			prompt+="User Query: $query $STOP_SYMBOL\n"
			prompt+="Your Super-Short Answer:"
		else
			prompt+="Your task is to assist the user in an interactive session, responding concisely and accurately."
		fi

		echo "$prompt"
	}

	# Extract content from a file (supports text and PDF files)
	function extract_content() {
		local file=$1 max_length=$2
		[[ ! -f "$file" ]] && { echo "Error: File not found." >&2; return 1; }

		local content
		case $file in
			*.pdf)
				command -v pdftotext >/dev/null 2>&1 || {
					echo "Error: pdftotext not installed. Install it using your package manager (e.g., brew install poppler)." >&2
					return 1
				}
				content=$(pdftotext "$file" - 2>/dev/null)
				;;
			*.txt | *)
				content=$(cat "$file")
				;;
		esac

		[[ -n "$max_length" && "$max_length" -gt 0 ]] && content=${content:0:$max_length}
		echo "$content"
	}

	# Start a llama-cli session
	function start_llama_session() {
		local repo_name=$1 model_name=$2 prompt=$3 interactive=$4 verbose=$5
		local args=(
			--threads "$(sysctl -n hw.logicalcpu_max)"
			--hf-repo "$repo_name"
			--hf-file "$model_name"
			--temp 0.2
			--prompt "$prompt"
			--seed 42
			--flash-attn
			--prio 3
			--mirostat 2
		)

		if [[ $interactive == true ]]; then
			args+=(--conversation --prompt-cache "$HOME/.yo_interactive_prompt_cache")
		else
			args+=(--reverse-prompt "$STOP_SYMBOL" --prompt-cache "$HOME/.yo_oneoff_prompt_cache")
		fi

		if [[ $verbose != true ]]; then
			llama-cli "${args[@]}" --no-display-prompt 2>/dev/null
		else
			llama-cli "${args[@]}" --verbose-prompt
		fi
	}

	# Parse arguments
	local read_mode=false read_file="" query="" verbose=false
	while [[ $# -gt 0 ]]; do
		case $1 in
			-h | --help) show_help; return 0 ;;
			-r | --read)
				read_mode=true
				[[ -n $2 && ! $2 =~ ^- ]] && { read_file=$2; shift; } || { echo "Error: --read requires a file." >&2; return 1; }
				;;
			-v | --verbose) verbose=true ;;
			*) query+="$1 " ;;
		esac
		shift
	done

	# Handle file reading
	local content=""
	if [[ $read_mode == true && -n "$read_file" ]]; then
		content=$(extract_content "$read_file" 10000) || return 1
	fi

  local quant_provider="bartowski" # Hugging Face username
	local model_series="Qwen2.5"        # Model series
	local tuning="Instruct"       # Model tuning
	local filetype="gguf"             # Model type

	# Configure model settings
	local quant size repo_name model_name
	if [[ -n "$query" ]]; then
		size="3B"
		quant="Q4_K_M"
	else
		size="14B"
		quant="IQ4_XS"
	fi
	repo_name="${quant_provider}/${model_series}-${size}-${tuning}-$(echo $filetype | tr '[:lower:]' '[:upper:]')"
	model_name="${model_series}-${size}-${tuning}-${quant}.${filetype}"

	# Generate and start appropriate session
	local prompt
	if [[ -n "$query" ]]; then
		prompt=$(generate_prompt "one-off" "$content" "$read_file" "$query")
		start_llama_session "$repo_name" "$model_name" "$prompt" false $verbose
	else
		prompt=$(generate_prompt "interactive" "$content" "$read_file")
		start_llama_session "$repo_name" "$model_name" "$prompt" true $verbose
	fi
}

yo say hi